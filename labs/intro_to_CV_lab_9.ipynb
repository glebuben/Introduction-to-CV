{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "wn7bC7OIqBbN",
        "aukHJSXSvxpG"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEhmtS9kDy60"
      },
      "outputs": [],
      "source": [
        "#importing everything required for the lab\n",
        "import numpy as np\n",
        "import time\n",
        "from tensorflow import keras\n",
        "from tqdm import tqdm\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Copying functions from previous labs"
      ],
      "metadata": {
        "id": "BKkTDKTjJYoJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolution"
      ],
      "metadata": {
        "id": "BWemJAqvJhem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# forward propagation for convolution\n",
        "def conv2d_forward(matrix, filter):\n",
        "  h_x, w_x, _ = matrix.shape # getting matrix shape\n",
        "  h_w, w_w, _  = filter.shape # getting filter shape\n",
        "  output = np.zeros((h_x - h_w + 1, w_x - w_w + 1)) # initializing output matrix\n",
        "  for i in range(len(output)): # for each pixel\n",
        "    for j in range(len(output[i])):\n",
        "        output[i][j] = np.sum(matrix[i:i+h_w, j:j+w_w, :] * filter) # calculate sum of hadamart product between\n",
        "                                                                    # matrix batch and filter, save value in output cell\n",
        "  return output"
      ],
      "metadata": {
        "id": "4jkHBWEmJXaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# backward propagation for convolution (dL/dZ)\n",
        "def conv2d_backward(upstream, filter):\n",
        "  h_w, w_w, d_w  = filter.shape # getting filter shape\n",
        "  padded_upstream = np.pad(upstream,((h_w - 1, h_w - 1), (w_w - 1, w_w - 1), (0, 0)), 'constant') # padding upstream matrix\n",
        "  rotated_filter = np.rot90(np.rot90(filter)) # rotate filter by 180 degree\n",
        "  dL_dZ = [] # initializing output\n",
        "  for i in range(d_w): # for each channel\n",
        "    dL_dZ.append(conv2d_forward(padded_upstream, rotated_filter[:, :, i, np.newaxis])) # adding dL/dZ\n",
        "  return np.array(dL_dZ)"
      ],
      "metadata": {
        "id": "Wt-vYIhvJlSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# backward propagation for convolution (dL/dW)\n",
        "def conv2d_backward_weights(weights, upstream):\n",
        "  h_x, w_x, d_x  = weights.shape # getting filter shape\n",
        "  dL_dZ = [] # initializing output\n",
        "  for i in range(d_x): # for each channel\n",
        "    dL_dZ.append(conv2d_forward(weights[:, :, i, np.newaxis], upstream)) # adding dL/dZ\n",
        "  return np.transpose(np.array(dL_dZ), (1, 2, 0))"
      ],
      "metadata": {
        "id": "NlsBN9tqJnBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ReLU"
      ],
      "metadata": {
        "id": "CuF8nrrlJnd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.array([[-0.00590765,  0.18932873],\n",
        "       [-0.32396051,  0.25586596],\n",
        "       [ 0.22358098,  0.02217555]])\n",
        "1 * (a > 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrttovS4E1LA",
        "outputId": "cb267f09-8715-44a8-ac39-7eb8ce22a2a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1],\n",
              "       [0, 1],\n",
              "       [1, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#function for RelU forward and backward propagation\n",
        "#taken from previous assignments\n",
        "def RelU_jacobian(input):\n",
        "  return 1 * (input > 0)\n",
        "\n",
        "def RelU_forward_prop(input):\n",
        "  return np.maximum(input, 0)\n",
        "\n",
        "def RelU_backward_prop(input, loss):\n",
        "  jac = RelU_jacobian(input) # finding jacobian for RelU according to input\n",
        "  return jac * np.array(loss)"
      ],
      "metadata": {
        "id": "H3Cz0h1LJ69I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matmul"
      ],
      "metadata": {
        "id": "jPLv8BwuJ27M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#functions for matmul backward and forward propagation from previous assignments\n",
        "def MatMul_forward_prop(matrix, input):\n",
        "  return np.array(matrix) @ np.array(input)\n",
        "\n",
        "#function that finds dL/dx\n",
        "def MatMul_backward_prop(matrix, loss):\n",
        "  return np.array(matrix).T @ np.array(loss)\n",
        "\n",
        "#function that finds dL/dW\n",
        "def MatMul_matrix_backward_prop(X, loss):\n",
        "  return np.array(loss) @ np.array(X).T"
      ],
      "metadata": {
        "id": "xsFTVfkDJ4do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SoftMax"
      ],
      "metadata": {
        "id": "jIM6vuJBJ7mg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function for softmax forward and backward propagation\n",
        "#taken from previous assignments\n",
        "def SoftMax_forward_prop(input, normalization=False):\n",
        "  output = np.array(input, dtype=np.longdouble)\n",
        "  if normalization: # if we use normalization\n",
        "    output = output - np.max(input) # we substract maximal value from each number\n",
        "  output = np.exp(output)\n",
        "  return output / np.sum(output)\n",
        "\n",
        "def SoftMax_jacobian(input, normalization=False): # function for calculating jacobian of SoftMax according to input\n",
        "  output = SoftMax_forward_prop(input, normalization)\n",
        "  jacobian = np.zeros((len(input), len(input)))\n",
        "  for i in range(len(input)):\n",
        "    for j in range(len(input)):\n",
        "      if i == j:\n",
        "        jacobian[i][j] = output[i] * (1 - output[j])\n",
        "      else:\n",
        "        jacobian[i][j] = -output[i] * output[j]\n",
        "  return jacobian\n",
        "\n",
        "def SoftMax_backward_prop(input, loss, normalization=False): # backpropagation\n",
        "  jac = SoftMax_jacobian(input, normalization) # calculating jacobian\n",
        "  return jac @ np.array(loss)"
      ],
      "metadata": {
        "id": "KFWemZTUJ9vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Log softmax"
      ],
      "metadata": {
        "id": "KIv7o1k4J_4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#functions for log_softmax forward and backward propagation\n",
        "#this node applies softmax and then finds logorithm of the result\n",
        "def log_softmax(x):\n",
        "  x_max = np.max(x)\n",
        "  return x - x_max - np.log(np.sum(np.exp(x - x_max)))\n",
        "\n",
        "def log_softmax_jacobian(input): # function for calculating jacobian of SoftMax according to input\n",
        "  output = SoftMax_forward_prop(input, True)\n",
        "  jacobian = np.zeros((len(input), len(input)))\n",
        "  for i in range(len(input)):\n",
        "    for j in range(len(input)):\n",
        "      if i == j:\n",
        "        jacobian[i][j] = (1 - output[j])\n",
        "      else:\n",
        "        jacobian[i][j] = -output[j]\n",
        "  return jacobian\n",
        "\n",
        "def log_softmax_backward_prop(input, loss): # backpropagation\n",
        "  jac = log_softmax_jacobian(input) # calculating jacobian\n",
        "  return jac @ np.array(loss)"
      ],
      "metadata": {
        "id": "jfppFkmSKCDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Labels vectorization"
      ],
      "metadata": {
        "id": "5k5dKRqdcFvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function taken from previous assignments\n",
        "#it translates label number into the vector of 0s and 1s\n",
        "def label_vec_func(labels):\n",
        "  labels_matrix = np.zeros([len(labels), c])\n",
        "  for i in range(len(labels)):\n",
        "    labels_matrix[i, labels[i]] = 1\n",
        "  return labels_matrix"
      ],
      "metadata": {
        "id": "565Ye-qfROCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing functions required for this lab"
      ],
      "metadata": {
        "id": "_c9vGHcEb6GJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# forward propagation for convolution\n",
        "# now we have filters - array of size: (height, width, depth, number of filters)\n",
        "def conv2d_forward_many(matrix, filters):\n",
        "  h_x, w_x, _ = matrix.shape # getting matrix shape\n",
        "  h_w, w_w, _, d  = filters.shape # getting filter shape\n",
        "  output = np.zeros((h_x - h_w + 1, w_x - w_w + 1, d)) # initializing output matrix\n",
        "  for k in range(d): #for each filter\n",
        "    output[:, :, k] = conv2d_forward(matrix, filters[:, :, :, k])\n",
        "  return output"
      ],
      "metadata": {
        "id": "dZEgEccRcBHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# backward propagation for convolution (dL/dZ)\n",
        "# now we have filters - array of size: (height, width, depth, number of filters)\n",
        "def conv2d_backward_many(upstream, filters):\n",
        "  h_w, w_w, d_w, D  = filters.shape # getting filter shape\n",
        "  dL_dZ = []\n",
        "  for i in range(D): # for each filter\n",
        "    dL_dZ.append(conv2d_backward(upstream[:, :, i, np.newaxis], filters[:, :, : , i]))\n",
        "  return np.transpose(np.sum(dL_dZ, 0), (1, 2, 0)) # transpose, since filters size is first after np.array"
      ],
      "metadata": {
        "id": "_2_VDSVeqhPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# backward propagation for convolution (dL/dW)\n",
        "# now we have filters - array of size: (height, width, depth, number of filters)\n",
        "def conv2d_backward_weights_many(weight, upstream):\n",
        "  _, _, D  = upstream.shape # getting filter shape\n",
        "  dL_dWs = [] # initializing output\n",
        "  for i in range(D): # for each channel\n",
        "    dL_dWs.append(conv2d_backward_weights(weight, upstream[:, :, i, np.newaxis]))\n",
        "  return np.transpose(np.array(dL_dWs), (1, 2, 3, 0)) # transpose, since filters size is first after np.array"
      ],
      "metadata": {
        "id": "GEKpsqIdqlvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing new function"
      ],
      "metadata": {
        "id": "wn7bC7OIqBbN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forward propagation"
      ],
      "metadata": {
        "id": "oBDucyQio48Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initializing input array and filter\n",
        "arr1d = np.array([[[1], [2]], [[3], [4]]])\n",
        "\n",
        "filt1d = np.array([[[[0.79559247, 0.520825  , 0.99224229]],\n",
        "\n",
        "        [[0.08872951, 0.47740497, 0.69036656]]]])"
      ],
      "metadata": {
        "id": "2rkMU_b3tJV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating convolution forward propagation using my function\n",
        "print(conv2d_forward_many(arr1d, filt1d))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E11FZ3-UtD5O",
        "outputId": "c50d83c0-bb13-470d-bb79-2a7f08c51484"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0.97305149 1.47563494 2.37297541]]\n",
            "\n",
            " [[2.74169545 3.47209488 5.73819311]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn.functional import conv2d"
      ],
      "metadata": {
        "id": "LukLraJMlXma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initializing input array and filter as torch tensors\n",
        "custom_filter = torch.tensor(filt1d.transpose(3,2,0,1), dtype=float, requires_grad=True)\n",
        "input_data = torch.tensor(arr1d, dtype=float, requires_grad=True)\n",
        "input_matrix = input_data.unsqueeze(3).permute(2, 3, 0, 1)"
      ],
      "metadata": {
        "id": "DtzquKpdh9mF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating convolution forward propagation using pytroch\n",
        "output = conv2d(input_matrix, custom_filter, padding=0)\n",
        "print(output.permute(0, 3, 2, 1).detach().numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRe0aqzkiENR",
        "outputId": "b2005c65-288a-49be-f9d4-4c9cdfaa7512"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[[0.97305149 1.47563494 2.37297541]\n",
            "   [2.74169545 3.47209488 5.73819311]]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backprop"
      ],
      "metadata": {
        "id": "aukHJSXSvxpG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\frac{∂\\bf L}{∂\\bf Z_{k-1}}$"
      ],
      "metadata": {
        "id": "Ov6VxEcQnZSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initializing input array and loss as torch tensors\n",
        "loss = torch.randn(1, 3, 3, 1, dtype=float,  requires_grad=True)\n",
        "\n",
        "\n",
        "input_matrix = torch.randn(1, 1, 3, 2, dtype=float,  requires_grad=True) # dL/dZ does not depend on input matrix according to lecture slides"
      ],
      "metadata": {
        "id": "t4zZpe3NvvwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_matrix.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJU8BoYjyYk8",
        "outputId": "ff33b224-2d97-408e-8b39-0cbd020e645b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 3, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating convolution forward propagation using my function\n",
        "out = conv2d_forward_many(input_matrix.permute(0,2,3,1).squeeze(0).detach().numpy(), filt1d)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtksMte2xnEg",
        "outputId": "28368556-31f1-435b-cc0f-2178c1bca9c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[-0.52044866 -1.17758066 -1.80606315]]\n",
            "\n",
            " [[-0.42567757 -0.64739822 -1.04066509]]\n",
            "\n",
            " [[-0.72806595 -1.09877945 -1.76815584]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDyshhfF89Ug",
        "outputId": "021cc751-5eb0-404e-defd-eb229f0fb02e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 1, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating convolution backward propagation using pytroch\n",
        "output = conv2d(input_matrix, custom_filter)\n",
        "print(output.permute(2, 3, 0, 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQbqhv7ywGkK",
        "outputId": "81113fdc-55f1-4858-8594-b3790a23cbd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[-0.5204, -1.1776, -1.8061]]],\n",
            "\n",
            "\n",
            "        [[[-0.4257, -0.6474, -1.0407]]],\n",
            "\n",
            "\n",
            "        [[[-0.7281, -1.0988, -1.7682]]]], dtype=torch.float64,\n",
            "       grad_fn=<PermuteBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss.permute(2, 3, 0, 1).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdJmJbsN0o_W",
        "outputId": "23f4fcb1-52a8-4c7e-ab92-f06db69ef2e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 1, 1, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output.backward(loss)\n",
        "input_matrix.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ynre4u90Nzr",
        "outputId": "16e05576-a457-43ef-952d-23dc6eac98b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[-1.8941, -1.3050],\n",
              "          [ 0.7315,  0.3237],\n",
              "          [-1.4082, -0.8771]]]], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss.permute(0, 2, 3, 1).squeeze(0).detach().numpy().shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygq5-tKS1lXp",
        "outputId": "ac853402-09e3-448f-ded3-e052c681ddbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 1, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating convolution forward propagation using my function\n",
        "print(conv2d_backward_many(loss.permute(0, 2, 3, 1).squeeze(0).detach().numpy(), filt1d))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sa41VmgzXUy",
        "outputId": "0be98f7d-b50d-4a61-bbaf-500bac8f0359"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[-1.89406093]\n",
            "  [-1.30497249]]\n",
            "\n",
            " [[ 0.73148394]\n",
            "  [ 0.32373761]]\n",
            "\n",
            " [[-1.40818568]\n",
            "  [-0.87711921]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### $\\frac{∂\\bf L}{∂\\bf W}$"
      ],
      "metadata": {
        "id": "df4DbnDwm_xp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_filter.grad.permute(2,3,1,0).detach().cpu().numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCD6QWBwkXNF",
        "outputId": "a175ff09-7faa-4a3e-b02c-2abae0882437"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[[0.20162918, 0.80112189, 0.98341681]],\n",
              "\n",
              "        [[0.70193302, 2.70520594, 3.28438387]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_matrix = input_matrix.detach().squeeze(0).permute(1, 2, 0).numpy()\n",
        "input_matrix = input_matrix"
      ],
      "metadata": {
        "id": "5SJdmCDXky1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = loss.permute(0, 2, 3, 1).squeeze(0).detach().numpy()"
      ],
      "metadata": {
        "id": "-wPFp-VN7_Tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating convolution forward propagation using my function\n",
        "a = conv2d_backward_weights_many(input_matrix, loss)\n",
        "print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmSF_tgkkXG0",
        "outputId": "5b51f263-4eaf-4df1-f404-c83ededa92d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[[0.20162918 0.80112189 0.98341681]]\n",
            "\n",
            "  [[0.70193302 2.70520594 3.28438387]]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading and preprocessing dataset"
      ],
      "metadata": {
        "id": "37LcyMiuM6jk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading cifar10 dataset\n",
        "dataset=keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = dataset.load_data() #loading data\n",
        "c = 5 # number of labels"
      ],
      "metadata": {
        "id": "uJm04JOcM53b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's decrease samples in dataset in order to speed up the training process"
      ],
      "metadata": {
        "id": "nrRwIVJ9Y5dV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decrease_mnist_samples(data, new_num_classes, max_repeat):\n",
        "  # decreasing dataset by number of classes and number of samples for each class\n",
        "  classes = range(new_num_classes) # classes that stayed\n",
        "  repeations = {i:0 for i in classes} # dictionary for repetitions\n",
        "  decreased_X, decreased_Y = [], [] # new X and Y\n",
        "  for x, y in data:\n",
        "    if y in classes: # for remained class\n",
        "      if repeations[y] < max_repeat: # check number of repetitions\n",
        "        repeations[y]+=1 # update this number\n",
        "        decreased_X.append(x) # add new value to X\n",
        "        decreased_Y.append(y) # add new value to Y\n",
        "  # shuffle new dataset\n",
        "  np.random.shuffle(decreased_X)\n",
        "  np.random.shuffle(decreased_Y)\n",
        "  return np.array(decreased_X), np.array(decreased_Y)"
      ],
      "metadata": {
        "id": "cIDvcrXtY4kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train)= decrease_mnist_samples(zip(x_train, y_train), c, 160)\n",
        "(x_test, y_test) = decrease_mnist_samples(zip(x_test, y_test), c, 20)\n",
        "y_train = label_vec_func(y_train) #converting labels to vector of 0s and 1s\n",
        "N_train = y_train.shape[0] #number of pictures in the train dataset\n",
        "N_test = y_test.shape[0] #number of pictures in the test dataset"
      ],
      "metadata": {
        "id": "YFM0k-yzadC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOUZkMleM0RW",
        "outputId": "11f13b94-ad5e-4990-c90e-eaf88600e069"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(800, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "h, w = 28, 28\n",
        "n = h * w #number of pixels for one picture"
      ],
      "metadata": {
        "id": "4AsA5JUEn63x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hw1, Ww1, d, D1 = 1, 1, 1, 2 # initialize kernel sizes, depth and number of filters for first layer\n",
        "\n",
        "hw2, Ww2, D2 = 1, 1, 2 # initialize kernel sizes, number of filters for second layer\n",
        "\n",
        "Conv1 =  np.random.uniform(-1, 1, (hw1, Ww1, d, D1)) # initial convolution filters for layer 1\n",
        "b1 = np.random.uniform(-1, 1 , (h - hw1 + 1, h - Ww1 + 1, D1)) #b1 - initial bias\n",
        "Conv2 =  np.random.uniform(-1, 1, (hw2, Ww2, D1, D2)) # initial convolution filters for layer 2\n",
        "b2 = np.random.uniform(-1, 1 , (h - hw1 - hw2 + 2, h - Ww1 - Ww2 + 2, D2)) #b2 - initial bias\n",
        "\n",
        "vectorized_len = (h - hw1 - hw2 + 2 ) * (h - Ww1 - Ww2 + 2) * D2 # size for vectorized tensor after 2 convulitions\n",
        "\n",
        "c1 = vectorized_len // 4 # dimension of the first layer\n",
        "W1 = np.random.uniform(-1, 1, (c1, vectorized_len)) #W1 - initial weights\n",
        "W2 = np.random.uniform(-1, 1, (c, c1)) #W2 - initial weights\n",
        "b3 = np.random.uniform(-1, 1 , (c1, 1)) #b1 - initial bias\n",
        "b4 = np.random.uniform(-1, 1 , (c, 1)) #b2 - initial bias\n",
        "\n",
        "\n",
        "nu = 0.005 # learning rate\n",
        "num_epochs = 10 # amount of epochs\n",
        "\n",
        "N = 2 # number of images in minibatch\n",
        "\n",
        "# initialize partial derivatives\n",
        "dL_dConv1 = np.zeros((h - hw1 + 1, h - Ww1 + 1, D1))\n",
        "dL_dConv2 = np.zeros((hw1, Ww1, d, D1))\n",
        "dL_dW2 = np.zeros((c1, vectorized_len))\n",
        "dL_dW2 = np.zeros((c, c1))\n",
        "dL_db1 = 0\n",
        "dL_db2 = 0\n",
        "dL_db3 = 0\n",
        "dL_db4 = 0"
      ],
      "metadata": {
        "id": "-XQDgSU-nQwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(num_epochs): #for each epoch\n",
        "  total_loss = 0 #sum of losses for one epoch\n",
        "  correct = 0 #number of correct predictions\n",
        "  counter = 0 #counter to check that batch ended\n",
        "  for i in tqdm(range(N_train)): #for each picture\n",
        "\n",
        "    x = x_train[i, :, : , np.newaxis] / 255 #normalize pixels, so they will be from 0 to 1\n",
        "\n",
        "    y_true = np.array(y_train[i].reshape(c,  1))  #true value of y\n",
        "\n",
        "    #forward propagation\n",
        "\n",
        "    y1 = conv2d_forward_many(x, Conv1) #applying convoltion filters from layer 1\n",
        "    y2 = y1 + b1 #adding bias\n",
        "    y3 = RelU_forward_prop(y2) #applying RelU\n",
        "    y4 =  conv2d_forward_many(y3, Conv2) #applying convoltion filters from layer 2\n",
        "    y5 = y4 + b2 #adding bias\n",
        "    y6 = RelU_forward_prop(y5) #applying RelU\n",
        "\n",
        "    y7 = np.reshape(y6, (vectorized_len, 1))\n",
        "\n",
        "    y8 = MatMul_forward_prop(W1, y7) #applying matrix multiplication with the first weight matrix\n",
        "    y9 = y8 + b3 #adding bias\n",
        "    y10 = RelU_forward_prop(y9) #applying RelU\n",
        "    y11 =  MatMul_forward_prop(W2, y10) #applying matrix multiplication with the second weight matrix\n",
        "    y12 = y11 + b4 #adding bias\n",
        "    y13 = log_softmax(y12) #applying log softmax\n",
        "\n",
        "    if np.argmax(y13) == np.argmax(y_true): #check if predictions are correct\n",
        "      correct += 1\n",
        "\n",
        "    loss = y_true.T @ y13 #finding loss\n",
        "    total_loss += loss #adding current loss to total loss\n",
        "\n",
        "    #backward propagation\n",
        "\n",
        "    back = -y_true + SoftMax_forward_prop(y12, True) #backpropagation from loss to the input of softmax\n",
        "\n",
        "    dL_db4 += back #backpropagation from loss to the input of addition, finding dL/db4\n",
        "\n",
        "    dL_dW2 += MatMul_matrix_backward_prop(y10, back) #fiding dL/dW2\n",
        "\n",
        "    back = MatMul_backward_prop(W2, back) #backpropagation from loss to the input of matrix multiplication with matrix W2\n",
        "\n",
        "    back = RelU_backward_prop(y9, back) #backpropagation from loss to the input of RelU\n",
        "\n",
        "    dL_db3 = back #backpropagation from loss to the input of addition, finding dL/db1\n",
        "\n",
        "    dL_dW1 = MatMul_matrix_backward_prop(y7, back) #fiding dL/dW1\n",
        "\n",
        "    back = MatMul_backward_prop(W1, back) #backpropagation from addition to the input of matrix multiplication with matrix W1\n",
        "\n",
        "    back = back.reshape((h - hw1 - hw2 + 2, h - Ww1 - Ww2 + 2, D2)) # backpropagation of reshaping\n",
        "\n",
        "    back = RelU_backward_prop(y5, back) # backpropagation from reshaping to the input of RelU\n",
        "\n",
        "    dL_db2 = back #backpropagation from ReLU to the input of addition, finding dL/db2\n",
        "\n",
        "    dL_dConv2 = conv2d_backward_weights_many(y3, back) #finding dL/dConv2\n",
        "\n",
        "    back = conv2d_backward_many(back, Conv2) # backpropagation from addition to the input convolution\n",
        "    back = RelU_backward_prop(y2, back) # backpropagation from convolution to the input of RelU\n",
        "\n",
        "    dL_db1 = back #backpropagation from ReLU to the input of addition, finding dL/db1\n",
        "\n",
        "    dL_dConv1 = conv2d_backward_weights_many(x, back) #finding dL/dConv1\n",
        "\n",
        "    counter += 1  # increasing counter\n",
        "\n",
        "    if counter == N or i == N_train - 1: # if batch ended or dataset ended. We apply gradient descent only in batches\n",
        "    #applying gradient descent for weights and biases\n",
        "\n",
        "      Conv1 = Conv1 - nu * dL_dConv1\n",
        "      Conv2 = Conv2 - nu * dL_dConv2\n",
        "      W1 = W1 - nu / N * dL_dW1\n",
        "      W2 = W2 - nu / N * dL_dW2\n",
        "      b1 = b1 - nu / N * dL_db1\n",
        "      b2 = b2 - nu / N * dL_db2\n",
        "      b3 = b3 - nu / N * dL_db3\n",
        "      b4 = b4 - nu / N * dL_db4\n",
        "\n",
        "      dL_dConv1 = np.zeros((h - hw1 + 1, h - Ww1 + 1, D1))\n",
        "      dL_dConv2 = np.zeros((hw1, Ww1, d, D1))\n",
        "      dL_dW2 = np.zeros((c1, vectorized_len))\n",
        "      dL_dW2 = np.zeros((c, c1))\n",
        "      dL_db1 = 0\n",
        "      dL_db2 = 0\n",
        "      dL_db3 = 0\n",
        "      dL_db4 = 0\n",
        "\n",
        "\n",
        "  print('\\nAccuracy:', correct / N_train)\n",
        "  print('Loss:', -total_loss.item() / N_train)\n"
      ],
      "metadata": {
        "id": "-y1DHX4dnxf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "680f37ea-6fc2-4abd-831b-434240297b75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [01:02<00:00, 12.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.49625\n",
            "Loss: 269.44150649173284698\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:58<00:00, 13.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.51625\n",
            "Loss: 102.33412987727251005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:58<00:00, 13.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.52125\n",
            "Loss: 96.545403509108815944\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:57<00:00, 13.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.52375\n",
            "Loss: 81.814858737146530225\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:56<00:00, 14.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.51625\n",
            "Loss: 70.326054228306836676\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:59<00:00, 13.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.515\n",
            "Loss: 60.800231269533709665\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:57<00:00, 14.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.5025\n",
            "Loss: 54.92806543720565775\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:57<00:00, 13.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.525\n",
            "Loss: 60.108655925068598665\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [01:06<00:00, 12.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.50125\n",
            "Loss: 92.106853230630311054\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:59<00:00, 13.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.5125\n",
            "Loss: 106.409671040141248735\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation, taken from lab 6"
      ],
      "metadata": {
        "id": "iC0a1UBlqRp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = np.zeros((N_test, 1), int) #predictions\n",
        "for i in tqdm(range(N_test)):\n",
        "    x = x_test[i, :, : , np.newaxis] / 255 #normalize pixels, so they will be from 0 to 1\n",
        "\n",
        "    y1 = conv2d_forward_many(x, Conv1) #applying matrix multiplication with the first weight matrix\n",
        "    y2 = y1 + b1 #adding bias\n",
        "    y3 = RelU_forward_prop(y2) #applying RelU\n",
        "    y4 =  conv2d_forward_many(y3, Conv2) #applying matrix multiplication with the second weight matrix\n",
        "    y5 = y4 + b2 #adding bias\n",
        "    y6 = RelU_forward_prop(y5) #applying RelU\n",
        "\n",
        "    y7 = np.reshape(y6, (vectorized_len, 1))\n",
        "\n",
        "    y8 = MatMul_forward_prop(W1, y7) #applying matrix multiplication with the first weight matrix\n",
        "    y9 = y8 + b3 #adding bias\n",
        "    y10 = RelU_forward_prop(y9) #applying RelU\n",
        "    y11 =  MatMul_forward_prop(W2, y10) #applying matrix multiplication with the second weight matrix\n",
        "    y12 = y11 + b4 #adding bias\n",
        "    out = SoftMax_forward_prop(12, True) #applying softmax to find outputs\n",
        "\n",
        "    y_pred[i] = np.argmax(out) #setting prediction (the greatest number among outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNyOlH9AyfyZ",
        "outputId": "f902f622-d3fb-47fa-e0af-187ffd5a2385"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:02<00:00, 36.33it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy using L1 distance:\", metrics.accuracy_score(y_true=y_test, y_pred=y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yZBUjEpymy_",
        "outputId": "b57a5e41-32b7-4b0c-c0da-a96824a511e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy using L1 distance: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Torch based model"
      ],
      "metadata": {
        "id": "-S9xDDoMqSTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class ConvolutionalNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.sequence = nn.Sequential(\n",
        "        nn.Conv2d(d, D1, (hw1, Ww1), padding=0, bias=True), # Convolution layer 1 with the same parameters and with bias\n",
        "        nn.ReLU(), # ReLU\n",
        "        nn.Conv2d(D1, D2, (hw2, Ww2), padding=0, bias=True), # Convolution layer 2 with the same parameters\n",
        "        nn.ReLU(), # RelU\n",
        "        nn.Flatten(), # Vectorization\n",
        "        nn.Linear(vectorized_len, c1), # Linear layer 1 with the same parameters, has bias by default\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(c1, c), # Linear layer 2 with the same parameters, has bias by default\n",
        "        nn.Softmax(dim=1) # Softmax\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "        return self.sequence(x)"
      ],
      "metadata": {
        "id": "mQDUW6K9U1g8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ConvolutionalNN() # initializing model\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=nu) # gradient descent with same learning rate\n",
        "loss_fn = nn.CrossEntropyLoss() # same loss"
      ],
      "metadata": {
        "id": "tF8z60DShTOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for i in range(num_epochs): #for each epoch\n",
        "  total_loss = 0 #sum of losses for one epoch\n",
        "  X_batch = []\n",
        "  Y_batch = []\n",
        "  for i in tqdm(range(N_train)): #for each picture\n",
        "\n",
        "    x = torch.tensor(x_train[i, np.newaxis, :, :] / 255, dtype=torch.float32)\n",
        "    y = torch.tensor(y_train[i], dtype=torch.float32)\n",
        "\n",
        "    if len(X_batch) > N or i == N_train - 1: # if batch ended\n",
        "      # convert list of samples to tensors\n",
        "      X = torch.stack(X_batch)\n",
        "      Y = torch.stack(Y_batch)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      output = model(X) # find output\n",
        "      loss = loss_fn(output, Y) # send to loss\n",
        "      total_loss += loss.item()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # clear batches\n",
        "      X_batch = []\n",
        "      Y_batch = []\n",
        "\n",
        "    else:\n",
        "      # add to batch\n",
        "      X_batch.append(x)\n",
        "      Y_batch.append(y)\n",
        "\n",
        "  print('Loss:', total_loss / N_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bUPEgE4ieb7",
        "outputId": "ac1a378f-0d80-4feb-831c-1103489ceb66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:00<00:00, 1114.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.23270041916519404\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:00<00:00, 1079.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.23115042701363564\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:01<00:00, 700.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.2306171888113022\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:01<00:00, 595.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.23036581657826902\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:00<00:00, 1126.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.23020051531493663\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:00<00:00, 1038.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.23013948187232017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:00<00:00, 940.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.23005800314247607\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:00<00:00, 867.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.23001396730542184\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:01<00:00, 699.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.229956674054265\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:01<00:00, 721.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.22992470040917395\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = np.zeros((N_test, 1), int) #predictions\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for i in range(N_test): # for each sample in test\n",
        "        x = torch.tensor(x_test[i, :].reshape(1, 1, 28, 28), dtype=torch.float32) # convert to tensor\n",
        "        y_pred[i] = torch.argmax(model(x)) # find output of a model"
      ],
      "metadata": {
        "id": "dRc3ZSH8oFo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy using L1 distance:\", metrics.accuracy_score(y_true=y_test, y_pred=y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gH_yHVDWz-pr",
        "outputId": "e6fcac32-9087-4c12-88b7-023f38e74d6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy using L1 distance: 0.55\n"
          ]
        }
      ]
    }
  ]
}