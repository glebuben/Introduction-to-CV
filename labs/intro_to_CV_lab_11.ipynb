{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "wn7bC7OIqBbN",
        "aukHJSXSvxpG"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEhmtS9kDy60"
      },
      "outputs": [],
      "source": [
        "#importing everything required for the lab\n",
        "import numpy as np\n",
        "import time\n",
        "from tensorflow import keras\n",
        "from tqdm import tqdm\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Copying functions from previous labs"
      ],
      "metadata": {
        "id": "BKkTDKTjJYoJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Compute padding"
      ],
      "metadata": {
        "id": "GfVKkm94KBfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def compute_pad_filt(input_size, output_size):\n",
        "  Hx, Wx = input_size\n",
        "  Hy, Wy = output_size\n",
        "\n",
        "  min_pad_h, min_pad_w = 1, 1 # minimal padding is (1, 1)\n",
        "\n",
        "  min_h, min_w = Hx-Hy+3, Wx-Wy+3 # for minimal padding kernel size is: min_h = Hx - Hy + 2 * pad[0] + 1, min_w = Wx - Wy + 2 * pad[1] + 1\n",
        "\n",
        "  if min_h < 1: # minimal size for height of a kernel = 1\n",
        "    min_pad_h = 1 + math.ceil((1-min_h) / 2) # add paddings so kernel height greater or equal to 1\n",
        "    min_h = min_h + (min_pad_h-1) * 2 # calculate new height\n",
        "\n",
        "\n",
        "  if min_w < 1: # minimal size for width of a kernel = 1\n",
        "    min_pad_w = 1 + math.ceil((1-min_w) / 2) # add paddings so kernel width greater or equal to 1\n",
        "    min_w = min_w + (min_pad_w-1) * 2  # calculate new height\n",
        "\n",
        "\n",
        "  return (min_pad_h, min_pad_w), (min_h, min_w)"
      ],
      "metadata": {
        "id": "rq7YIfJyKBF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolution"
      ],
      "metadata": {
        "id": "BWemJAqvJhem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# forward propagation for convolution\n",
        "# add padings\n",
        "def conv2d_forward(matrix, filter, pad=(0,0)):\n",
        "  if pad != (0,0): # if there are paddings, apply them\n",
        "    matrix = np.pad(matrix, ((pad[0], pad[0]),(pad[1], pad[1]),(0,0)))\n",
        "  h_x, w_x, _ = matrix.shape # getting matrix shape\n",
        "  h_w, w_w, _  = filter.shape # getting filter shape\n",
        "  output = np.zeros((h_x - h_w + 1, w_x - w_w + 1)) # initializing output matrix\n",
        "  for i in range(len(output)): # for each pixel\n",
        "    for j in range(len(output[i])):\n",
        "        output[i][j] = np.sum(matrix[i:i+h_w, j:j+w_w, :] * filter) # calculate sum of hadamart product between\n",
        "                                                                    # matrix batch and filter, save value in output cell\n",
        "  return output"
      ],
      "metadata": {
        "id": "4jkHBWEmJXaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# backward propagation for convolution (dL/dZ)\n",
        "def conv2d_backward_pad(upstream, filter, pad=(0,0)):\n",
        "  # if there are paddings, we send them to conv2d_forward\n",
        "  h_w, w_w, d_w  = filter.shape # getting filter shape\n",
        "  rotated_filter = np.rot90(np.rot90(filter)) # rotate filter by 180 degree\n",
        "  dL_dZ = [] # initializing output\n",
        "  for i in range(d_w): # for each channel\n",
        "    dL_dZ.append(conv2d_forward(upstream, rotated_filter[:, :, i, np.newaxis], pad)) # adding dL/dZ\n",
        "  return np.array(dL_dZ)"
      ],
      "metadata": {
        "id": "Wt-vYIhvJlSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# backward propagation for convolution (dL/dW)\n",
        "def conv2d_backward_weights(weights, upstream, pad=(0,0)):\n",
        "   # if there are paddings, apply them\n",
        "  if pad != (0,0):\n",
        "    weights = np.pad(weights, ((pad[0], pad[0]),(pad[1], pad[1]),(0,0)))\n",
        "  h_x, w_x, d_x  = weights.shape # getting filter shape\n",
        "  dL_dZ = [] # initializing output\n",
        "  for i in range(d_x): # for each channel\n",
        "    dL_dZ.append(conv2d_forward(weights[:, :, i, np.newaxis], upstream)) # adding dL/dZ\n",
        "  return np.transpose(np.array(dL_dZ), (1, 2, 0))"
      ],
      "metadata": {
        "id": "NlsBN9tqJnBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ReLU"
      ],
      "metadata": {
        "id": "CuF8nrrlJnd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function for RelU forward and backward propagation\n",
        "#taken from previous assignments\n",
        "def RelU_jacobian(input):\n",
        "  return 1 * (input > 0)\n",
        "\n",
        "def RelU_forward_prop(input):\n",
        "  return np.maximum(input, 0)\n",
        "\n",
        "def RelU_backward_prop(input, loss):\n",
        "  jac = RelU_jacobian(input) # finding jacobian for RelU according to input\n",
        "  return jac * np.array(loss)"
      ],
      "metadata": {
        "id": "H3Cz0h1LJ69I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matmul"
      ],
      "metadata": {
        "id": "jPLv8BwuJ27M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#functions for matmul backward and forward propagation from previous assignments\n",
        "def MatMul_forward_prop(matrix, input):\n",
        "  return np.array(matrix) @ np.array(input)\n",
        "\n",
        "#function that finds dL/dx\n",
        "def MatMul_backward_prop(matrix, loss):\n",
        "  return np.array(matrix).T @ np.array(loss)\n",
        "\n",
        "#function that finds dL/dW\n",
        "def MatMul_matrix_backward_prop(X, loss):\n",
        "  return np.array(loss) @ np.array(X).T"
      ],
      "metadata": {
        "id": "1exvfK7jOtzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Labels vectorization"
      ],
      "metadata": {
        "id": "5k5dKRqdcFvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function taken from previous assignments\n",
        "#it translates label number into the vector of 0s and 1s\n",
        "def label_vec_func(labels):\n",
        "  labels_matrix = np.zeros([len(labels), c])\n",
        "  for i in range(len(labels)):\n",
        "    labels_matrix[i, labels[i]] = 1\n",
        "  return labels_matrix"
      ],
      "metadata": {
        "id": "565Ye-qfROCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolution for many filters"
      ],
      "metadata": {
        "id": "RB0Kpd03IkGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# forward propagation for convolution\n",
        "# now we have filters - array of size: (height, width, depth, number of filters)\n",
        "# add paddings to the function calls\n",
        "def conv2d_forward_many(matrix, filters, pad=(0,0)):\n",
        "  h_x, w_x, _ = matrix.shape # getting matrix shape\n",
        "  h_w, w_w, _, d  = filters.shape # getting filter shape\n",
        "  output = np.zeros((h_x - h_w + 1 + 2 *pad[0], w_x - w_w + 1 + 2 * pad[1], d)) # initializing output matrix\n",
        "  for k in range(d): #for each filter\n",
        "    output[:, :, k] = conv2d_forward(matrix, filters[:, :, :, k], pad)\n",
        "  return output"
      ],
      "metadata": {
        "id": "dZEgEccRcBHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# backward propagation for convolution (dL/dZ)\n",
        "# now we have filters - array of size: (height, width, depth, number of filters)\n",
        "# add paddings to the function calls\n",
        "def conv2d_backward_many(upstream, filters, pad=(0,0)):\n",
        "  h_w, w_w, d_w, D  = filters.shape # getting filter shape\n",
        "  dL_dZ = []\n",
        "  for i in range(D):  # for each filter\n",
        "    dL_dZ.append(conv2d_backward_pad(upstream[:, :, i, np.newaxis], filters[:, :, : , i], pad))\n",
        "  return np.transpose(np.sum(dL_dZ, 0), (1, 2, 0))"
      ],
      "metadata": {
        "id": "_2_VDSVeqhPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# backward propagation for convolution (dL/dW)\n",
        "# now we have filters - array of size: (height, width, depth, number of filters)\n",
        "# add paddings to the function calls\n",
        "def conv2d_backward_weights_many(weight, upstream, pad=(0,0)):\n",
        "  _, _, D  = upstream.shape # getting filter shape\n",
        "  dL_dWs = [] # initializing output\n",
        "  for i in range(D): # for each channel\n",
        "    dL_dWs.append(conv2d_backward_weights(weight, upstream[:, :, i, np.newaxis], pad))\n",
        "  return np.transpose(np.array(dL_dWs), (1, 2, 3, 0))"
      ],
      "metadata": {
        "id": "GEKpsqIdqlvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resize"
      ],
      "metadata": {
        "id": "ptzO13bgfp4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for rescaling I used Nearest-neighbour interpolation\n",
        "def nearest(input, width, height):\n",
        "  \"\"\"Function that takes photo, rescales it to specified width and height, given as a parameters\n",
        "\n",
        "    Reference to the source that I used: https://kwojcicki.github.io/blog/NEAREST-NEIGHBOUR\n",
        "\n",
        "    Keyword arguments:\n",
        "      input (np.array): input image represented as numpy array\n",
        "      width (int) -- width of a new rescaled image\n",
        "      height (int) -- height of a new rescaled image\n",
        "\n",
        "    Returns:\n",
        "      output (np.array): output (rescaled) image represented as numpy array\n",
        "  \"\"\"\n",
        "  # initialization of output variable\n",
        "  output = np.zeros((width, height, 3), dtype=int)\n",
        "  # calculation of scales between input's width, height and\n",
        "  sx = input.shape[0] / output.shape[0]\n",
        "  sy = input.shape[1] / output.shape[1]\n",
        "  # for each pixel from output image\n",
        "  for y in range(len(output)):\n",
        "    for x in range(len(output[y])):\n",
        "      # finding nearest input's pixel for current output's pixel\n",
        "      proj_x = math.floor(x * sx)\n",
        "      proj_y = math.floor(y * sy)\n",
        "\n",
        "      # initialize output's pixel using obtained nearest input's pixel\n",
        "      output[y][x] = input[proj_y][proj_x]\n",
        "\n",
        "  return output"
      ],
      "metadata": {
        "id": "jZdk6_mwfpRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing functions required for this lab"
      ],
      "metadata": {
        "id": "_c9vGHcEb6GJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SE_forward(y_pred, y_true):\n",
        "  output = y_pred - y_true\n",
        "  return np.sum(output ** 2)"
      ],
      "metadata": {
        "id": "JLnfZov0Iw4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def SE_backward(y_pred, y_true):\n",
        "  output = y_pred - y_true\n",
        "  return 2 * output"
      ],
      "metadata": {
        "id": "gUZq3vOMJvtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading and preprocessing dataset"
      ],
      "metadata": {
        "id": "MOnFAhPQXyjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests, zipfile\n",
        "\n",
        "link = 'https://www.dropbox.com/scl/fi/ha6f9n5rcae6h79dhdykb/pikachu.zip?rlkey=b1z9xqub0z4acglomt9r983q0&dl=1'\n",
        "file_path = 'dataset.zip'\n",
        "response = requests.get(link)\n",
        "\n",
        "with open(file_path, 'wb') as local_file:\n",
        "    local_file.write(response.content)\n",
        "\n",
        "with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('.')\n"
      ],
      "metadata": {
        "id": "pJh5H5oiJS0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function for reading dataset from directory"
      ],
      "metadata": {
        "id": "IcebvHv4eI1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "def read_dataset(directory, h, w):\n",
        "  annotation = eval(open(directory + '/annotations.json').read())\n",
        "  x = []\n",
        "  y = []\n",
        "  for data in annotation.values():\n",
        "    path = data['image']\n",
        "    box  = data['loc']\n",
        "    y.append(np.array(box))\n",
        "    image = cv2.imread(directory +'/images/' + path)\n",
        "    image = nearest(image, h, w)\n",
        "    x.append(image)\n",
        "  return np.array(x), np.array(y)"
      ],
      "metadata": {
        "id": "p4cBrTJRZRe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h, w = 32, 32\n",
        "n = h * w #number of pixels for one picture"
      ],
      "metadata": {
        "id": "4AsA5JUEn63x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train = read_dataset('/content/train', h, w)\n",
        "x_test, y_test = read_dataset('/content/val', h, w)"
      ],
      "metadata": {
        "id": "coiYF8jGfZ2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = 4 # number of outputs for last linear layer"
      ],
      "metadata": {
        "id": "8JJH4IAKKagu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_train, N_test = len(x_train), len(x_test)"
      ],
      "metadata": {
        "id": "evm0c63ud3IC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's decrease samples in dataset in order to speed up the training process"
      ],
      "metadata": {
        "id": "nrRwIVJ9Y5dV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pad, kernel = compute_pad_filt((h, w), (h, w)) # we want output to have the smae dimensions\n",
        "\n",
        "hw1, Ww1  = kernel # initialize kernel sizes for first layer\n",
        "d, D1 = 3, 2 # initialize depth and number of filters for first layer\n",
        "\n",
        "hw2, Ww2, D2 = hw1, Ww1, 2 # initialize kernel sizes, number of filters for second layer\n",
        "\n",
        "Conv1 =  np.random.uniform(-1, 1, (hw1, Ww1, d, D1)) # initial convolution filters for layer 1\n",
        "b1 = np.random.uniform(-1, 1 , (h, w, D1)) #b1 - initial bias\n",
        "Conv2 =  np.random.uniform(-1, 1, (hw2, Ww2, D1, D2))# initial convolution filters for layer 2\n",
        "b2 = np.random.uniform(-1, 1 , (h, w, D2)) #b2 - initial bias\n",
        "\n",
        "vectorized_len = h * w * D2 # size for vectorized tensor after 2 convulitions\n",
        "\n",
        "c1 = vectorized_len // 4 # dimension of the first layer\n",
        "W1 = np.random.uniform(-1, 1, (c1, vectorized_len)) #W1 - initial weights\n",
        "W2 = np.random.uniform(-1, 1, (c, c1)) #W2 - initial weights\n",
        "b3 = np.random.uniform(-1, 1 , (c1, 1)) #b1 - initial bias\n",
        "b4 = np.random.uniform(-1, 1 , (c, 1)) #b2 - initial bias\n",
        "\n",
        "\n",
        "nu = 0.0001 # learning rate\n",
        "num_epochs = 10 # amount of epochs\n",
        "\n",
        "N = 2 # number of images in minibatch\n",
        "\n",
        "# initialize partial derivatives\n",
        "dL_dConv1 = np.zeros((hw1, Ww1, d, D1))\n",
        "dL_dConv2 = np.zeros((hw2, Ww2, D1, D2))\n",
        "dL_dW1 = np.zeros((c1, vectorized_len))\n",
        "dL_dW2 = np.zeros((c, c1))\n",
        "dL_db1 = 0\n",
        "dL_db2 = 0\n",
        "dL_db3 = 0\n",
        "dL_db4 = 0"
      ],
      "metadata": {
        "id": "-XQDgSU-nQwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(num_epochs): #for each epoch\n",
        "  total_loss = 0 #sum of losses for one epoch\n",
        "  counter = 0 #counter to check that batch ended\n",
        "  for i in tqdm(range(N_train)): #for each picture\n",
        "\n",
        "    x = x_train[i, :, :] / 255 #normalize pixels, so they will be from 0 to 1\n",
        "\n",
        "    y_true = y_train[i].reshape(c, 1)\n",
        "    #forward propagation\n",
        "    y1 = conv2d_forward_many(x, Conv1, pad=pad) #applying convoltion filters from layer 1\n",
        "    y2 = y1 + b1 #adding bias\n",
        "    y3 = RelU_forward_prop(y2) #applying RelU\n",
        "    y4 =  conv2d_forward_many(y3, Conv2, pad=pad) #applying convoltion filters from layer 2\n",
        "    y5 = y4 + b2 #adding bias\n",
        "    y6 = RelU_forward_prop(y5) #applying RelU\n",
        "\n",
        "    y7 = np.reshape(y6, (vectorized_len, 1))\n",
        "\n",
        "    y8 = MatMul_forward_prop(W1, y7) #applying matrix multiplication with the first weight matrix\n",
        "    y9 = y8 + b3 #adding bias\n",
        "    y10 = RelU_forward_prop(y9) #applying RelU\n",
        "    y11 =  MatMul_forward_prop(W2, y10) #applying matrix multiplication with the second weight matrix\n",
        "    y12 = y11 + b4 #adding bias\n",
        "    # no ReLU after bias addition, since we have nonlinearity further (softmax)\n",
        "    loss = SE_forward(y12, y_true)\n",
        "\n",
        "    total_loss += loss #adding current loss to total loss\n",
        "\n",
        "    #backward propagation\n",
        "\n",
        "    back = SE_backward(y12, y_true) #backpropagation from loss to the input of softmax\n",
        "\n",
        "    dL_db4 += back #backpropagation from loss to the input of addition, finding dL/db4\n",
        "\n",
        "    dL_dW2 += MatMul_matrix_backward_prop(y10, back) #fiding dL/dW2\n",
        "\n",
        "    back = MatMul_backward_prop(W2, back) #backpropagation from loss to the input of matrix multiplication with matrix W2\n",
        "\n",
        "    back = RelU_backward_prop(y9, back) #backpropagation from loss to the input of RelU\n",
        "\n",
        "    dL_db3 += back #backpropagation from loss to the input of addition, finding dL/db3\n",
        "\n",
        "    dL_dW1 += MatMul_matrix_backward_prop(y7, back) #fiding dL/dW1\n",
        "\n",
        "    back = MatMul_backward_prop(W1, back) #backpropagation from addition to the input of matrix multiplication with matrix W1\n",
        "\n",
        "    back = back.reshape((h, w, D2)) # backpropagation of reshaping\n",
        "\n",
        "    back = RelU_backward_prop(y5, back) # backpropagation from reshaping to the input of RelU\n",
        "\n",
        "    dL_db2 += back #backpropagation from ReLU to the input of addition, finding dL/db2\n",
        "\n",
        "    dL_dConv2 += conv2d_backward_weights_many(y3, back, pad=pad) #finding dL/dConv2\n",
        "\n",
        "    back = conv2d_backward_many(back, Conv2, pad=pad) # backpropagation from addition to the input convolution\n",
        "    back = RelU_backward_prop(y2, back) # backpropagation from convolution to the input of RelU\n",
        "\n",
        "    dL_db1 += back #backpropagation from ReLU to the input of addition, finding dL/db1\n",
        "    dL_dConv1 += conv2d_backward_weights_many(x, back, pad=(1,1)) #finding dL/dConv1\n",
        "\n",
        "    counter += 1 # increasing counter\n",
        "\n",
        "    if counter == N or i == N_train - 1: # if batch ended or dataset ended. We apply gradient descent only in batches\n",
        "    #applying gradient descent for weights and biases\n",
        "\n",
        "      Conv1 = Conv1 - nu / N * dL_dConv1\n",
        "      Conv2 = Conv2 - nu / N * dL_dConv2\n",
        "      W1 = W1 - nu / N * dL_dW1\n",
        "      W2 = W2 - nu / N * dL_dW2\n",
        "      b1 = b1 - nu / N * dL_db1\n",
        "      b2 = b2 - nu / N * dL_db2\n",
        "      b3 = b3 - nu / N * dL_db3\n",
        "      b4 = b4 - nu / N * dL_db4\n",
        "\n",
        "      # setting partial derivatives to 0\n",
        "      dL_dConv1 = np.zeros((hw1, Ww1, d, D1))\n",
        "      dL_dConv2 = np.zeros((hw2, Ww2, D1, D2))\n",
        "      dL_dW1 = np.zeros((c1, vectorized_len))\n",
        "      dL_dW2 = np.zeros((c, c1))\n",
        "      dL_db1 = 0\n",
        "      dL_db2 = 0\n",
        "      dL_db3 = 0\n",
        "      dL_db4 = 0\n",
        "\n",
        "      counter = 0\n",
        "\n",
        "  print('\\nLoss:', total_loss / N_train)"
      ],
      "metadata": {
        "id": "-y1DHX4dnxf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "outputId": "d53b3f2d-f193-459f-8ec4-0fcb6461a7b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 8/900 [00:01<04:17,  3.46it/s]<ipython-input-8-07d8f6208191>:3: RuntimeWarning: overflow encountered in matmul\n",
            "  return np.array(matrix) @ np.array(input)\n",
            "<ipython-input-8-07d8f6208191>:11: RuntimeWarning: invalid value encountered in matmul\n",
            "  return np.array(loss) @ np.array(X).T\n",
            "<ipython-input-8-07d8f6208191>:7: RuntimeWarning: invalid value encountered in matmul\n",
            "  return np.array(matrix).T @ np.array(loss)\n",
            "<ipython-input-7-5ce665b1baed>:11: RuntimeWarning: invalid value encountered in multiply\n",
            "  return jac * np.array(loss)\n",
            "  3%|▎         | 28/900 [00:08<04:16,  3.41it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-6a892f420048>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mdL_dW1\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mMatMul_matrix_backward_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#fiding dL/dW1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMatMul_backward_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mback\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#backpropagation from addition to the input of matrix multiplication with matrix W1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# backpropagation of reshaping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-07d8f6208191>\u001b[0m in \u001b[0;36mMatMul_backward_prop\u001b[0;34m(matrix, loss)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#function that finds dL/dx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mMatMul_backward_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#function that finds dL/dW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "jz2mlHV46Vth"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For evaluation I will use intersection over union. The implementation is taken from this [source](https://pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/)"
      ],
      "metadata": {
        "id": "a2SoMfrO6Yhx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bb_intersection_over_union(boxA, boxB):\n",
        "\t# determine the (x, y)-coordinates of the intersection rectangle\n",
        "\txA = max(boxA[0], boxB[0])\n",
        "\tyA = max(boxA[1], boxB[1])\n",
        "\txB = min(boxA[2], boxB[2])\n",
        "\tyB = min(boxA[3], boxB[3])\n",
        "\t# compute the area of intersection rectangle\n",
        "\tinterArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
        "\t# compute the area of both the prediction and ground-truth\n",
        "\t# rectangles\n",
        "\tboxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
        "\tboxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
        "\t# compute the intersection over union by taking the intersection\n",
        "\t# area and dividing it by the sum of prediction + ground-truth\n",
        "\t# areas - the interesection area\n",
        "\tiou = interArea / float(boxAArea + boxBArea - interArea)\n",
        "\t# return the intersection over union value\n",
        "\treturn iou"
      ],
      "metadata": {
        "id": "Vr5inm6Y58lV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_iou = 0\n",
        "total_loss = 0\n",
        "for i in tqdm(range(N_test)):\n",
        "    x = x_test[i, :, :] / 255 #normalize pixels, so they will be from 0 to 1\n",
        "    y_true = y_test[i].reshape(c, 1)\n",
        "\n",
        "\n",
        "    y1 = conv2d_forward_many(x, Conv1, pad=pad) #applying convoltion filters from layer 1\n",
        "    y2 = y1 + b1 #adding bias\n",
        "    y3 = RelU_forward_prop(y2) #applying RelU\n",
        "    y4 =  conv2d_forward_many(y3, Conv2, pad=pad) #applying convoltion filters from layer 2\n",
        "    y5 = y4 + b2 #adding bias\n",
        "    y6 = RelU_forward_prop(y5) #applying RelU\n",
        "\n",
        "    y7 = np.reshape(y6, (vectorized_len, 1))\n",
        "\n",
        "    y8 = MatMul_forward_prop(W1, y7) #applying matrix multiplication with the first weight matrix\n",
        "    y9 = y8 + b3 #adding bias\n",
        "    y10 = RelU_forward_prop(y9) #applying RelU\n",
        "    y11 =  MatMul_forward_prop(W2, y10) #applying matrix multiplication with the second weight matrix\n",
        "    y_pred = y11 + b4 #adding bias\n",
        "    # no ReLU after bias addition, since we have nonlinearity further (softmax)\n",
        "    loss = SE_forward(y_pred, y_true)\n",
        "\n",
        "    total_loss += loss #adding current loss to total loss\n",
        "\n",
        "    # Define two bounding boxes as (x, y, w, h)\n",
        "    iou = bb_intersection_over_union(y_pred, y_true)\n",
        "    total_iou += iou"
      ],
      "metadata": {
        "id": "PNyOlH9AyfyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Mean intersection over union:\", total_iou.item() / N_test)\n",
        "print(\"Mean loss:\", total_loss / N_test)"
      ],
      "metadata": {
        "id": "4yZBUjEpymy_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}