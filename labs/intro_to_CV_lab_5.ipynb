{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "w0UG4F67rowO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1) Softmax function"
      ],
      "metadata": {
        "id": "6NhiFyPt1rFJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Softmax forward and back propagation implementation from previous lab seems to be incorrect. Therefore, in the first part let's rewirete these functions."
      ],
      "metadata": {
        "id": "uK8b4RdU14Au"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AzgAFWFQx-cY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#loss function required for backward propagation\n",
        "#in my case I use function that returns array with 1 for biggest number, and 0s for others\n",
        "def loss_fn(array):\n",
        "  max_val = np.argmax(array)\n",
        "  out = np.zeros(len(array))\n",
        "  out[max_val] = 1\n",
        "  return out"
      ],
      "metadata": {
        "id": "3hz3gT_G-bYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0d4L4I8rhk6"
      },
      "outputs": [],
      "source": [
        "def SoftMax_forward_prop(input, normalization=False):\n",
        "  output = input\n",
        "  if normalization: # if we use normalization\n",
        "    output -= np.max(input) # we substract maximal value from each number\n",
        "  output = np.exp(output)\n",
        "  return output / np.sum(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jacobian is calculated according to this source:\n",
        "\n",
        "https://aimatters.wordpress.com/2019/06/17/the-softmax-function-derivative/"
      ],
      "metadata": {
        "id": "A-wJ-lKKyATo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SoftMax_jacobian(input, normalization=False): # function for calculating jacobian of SoftMax according to input\n",
        "  output = SoftMax_forward_prop(input, normalization)\n",
        "  jacobian = np.zeros((len(input), len(input)))\n",
        "  for i in range(len(input)):\n",
        "    for j in range(len(input)):\n",
        "      if i == j:\n",
        "        jacobian[i][j] = output[i] * (1 - output[j])\n",
        "      else:\n",
        "        jacobian[i][j] = -output[i] * output[j]\n",
        "  return jacobian"
      ],
      "metadata": {
        "id": "jMcz7yXzsrjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def SoftMax_backward_prop(input, loss, normalization=False): # backpropagation\n",
        "  jac = SoftMax_jacobian(input, normalization) # calculating jacobian\n",
        "  return jac @ np.array(loss)"
      ],
      "metadata": {
        "id": "GnX0HGS9xB5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2) RelU function"
      ],
      "metadata": {
        "id": "21w9puWY4AlJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding jacobian of RelU:\n",
        "$$\\frac{\\partial z}{\\partial x}=\n",
        "\\begin{bmatrix}\n",
        "\\frac{\\partial z(x_1)}{∂x_1} & \\frac{\\partial z(x_1)}{∂x_2} & ... & \\frac{\\partial z(x_1)}{∂x_m} \\\\\n",
        " \\frac{\\partial z(x_2)}{∂x_1} & \\frac{\\partial z(x_2)}{∂x_2} & ... & \\frac{\\partial z(x_2)}{∂x_m}\\\\\n",
        " ...\n",
        " \\\\\\frac{\\partial z(x_m)}{∂x_1} & \\frac{\\partial z(x_m)}{∂x_2} & ... & \\frac{\\partial z(x_m)}{∂x_m}\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "Let's consider $\\frac{\\partial z(x_i)}{\\partial x_j}$.\n",
        "* If $i\\neq j$,  $\\frac{\\partial z(x_i)}{\\partial x_j}=0$.\n",
        "* If $i=j$ and $x_i > 0$: $\\frac{\\partial z(x_i)}{\\partial x_j} = \\frac{\\partial (max(0, x_i))}{\\partial x_i} = \\frac{\\partial x_i}{\\partial x_i} = 1$  \n",
        "* If $i=j$ and $x_i ≤ 0$: $\\frac{\\partial z(x_i)}{\\partial x_j} = \\frac{\\partial (max(0, x_i))}{\\partial x_i} = \\frac{\\partial (0)}{\\partial x_i} = 0$"
      ],
      "metadata": {
        "id": "gHcZVQ4MauUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RelU_jacobian(input):\n",
        "  jac = np.zeros((len(input), len(input)))\n",
        "  for i in range(len(input)):\n",
        "    if input[i] > 0:\n",
        "      jac[i][i] = 1\n",
        "  return jac"
      ],
      "metadata": {
        "id": "_lHE8muD4uNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RelU_forward_prop(input):\n",
        "  return np.array([max(0, x) for x in input]) # applying RelU to the input"
      ],
      "metadata": {
        "id": "T7uqjkH24FmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RelU_backward_prop(input, loss):\n",
        "  jac = RelU_jacobian(input) # finding jacobian for RelU according to input\n",
        "  return jac @ np.array(loss)"
      ],
      "metadata": {
        "id": "7LqAjUAL6Tw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3) Matrix multiplication"
      ],
      "metadata": {
        "id": "vLAaBV9I7jxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MatMul_forward_prop(matrix, input):\n",
        "  return np.array(matrix) @ np.array(input)"
      ],
      "metadata": {
        "id": "f5qP9ipE7u7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MatMul_backward_prop(matrix, loss):\n",
        "  return np.array(matrix).T @ np.array(loss)"
      ],
      "metadata": {
        "id": "aPJnlIVZ8WcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Let's test these functions"
      ],
      "metadata": {
        "id": "qxchmtPlnhnB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Forward propagation"
      ],
      "metadata": {
        "id": "5kPl33nq83qx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "a = [1., 9., 3.] # variable for my functions\n",
        "a_t1 = torch.tensor(a, requires_grad=True) # variable for torch SoftMax\n",
        "a_t2 = torch.tensor(a, requires_grad=True) # variabel for torch RelU"
      ],
      "metadata": {
        "id": "B6TdBsiY-UVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = SoftMax_forward_prop(a) # applying softmax forward propagation"
      ],
      "metadata": {
        "id": "Doftv73a-Vey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# comparing results\n",
        "print('Mine softmax forward output:', f)\n",
        "print('Torch softmax forward output:', nn.Softmax(dim=0).forward(a_t1).tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtwitMaQ9ikB",
        "outputId": "3d16c2d0-9158-4d02-8b0a-748b42a748db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mine softmax forward output: [0.1141952  0.04201007 0.84379473]\n",
            "Torch softmax forward output: [0.11419519037008286, 0.04201006516814232, 0.8437947034835815]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = RelU_forward_prop(a) # applying relu forward propagation"
      ],
      "metadata": {
        "id": "KLcya3IR9gfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# comparing results\n",
        "print('Mine softmax forward output:', f)\n",
        "print('Torch softmax forward output:', nn.ReLU().forward(a_t2).tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LDkJjjAoZTF",
        "outputId": "09f32a2d-4f63-4684-8280-f55700207eed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mine softmax forward output: [1. 0. 3.]\n",
            "Torch softmax forward output: [1.0, 0.0, 3.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W = np.random.rand(4, 3) # initialize matrix W for matrix multiplication\n",
        "X = [[1, 2], # initialize matrix X for matrix multiplication\n",
        "     [2, -3],\n",
        "     [-7, 9]]\n",
        "\n",
        "W_t = torch.tensor(W, dtype=torch.float64) # converting W to tensor\n",
        "X_t = torch.tensor(X, dtype=torch.float64, requires_grad=True) # converting X"
      ],
      "metadata": {
        "id": "c-qSv06G9VjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = MatMul_forward_prop(W, X) # applying matrix multiplication forward propagation"
      ],
      "metadata": {
        "id": "DOF8o5cI-Ndr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# comparing results\n",
        "print('Mine softmax forward output:\\n', f)\n",
        "print('Torch softmax forward output:\\n', torch.matmul(W_t, X_t).detach().numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZrlA3k0-Kh2",
        "outputId": "cad7b79b-fd93-4714-ba08-eb92d908cf6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mine softmax forward output:\n",
            " [[-4.45251977  6.65426244]\n",
            " [-0.70628076  3.28919234]\n",
            " [ 0.3575458   1.74498934]\n",
            " [ 0.28000093 -0.58458918]]\n",
            "Torch softmax forward output:\n",
            " [[-4.45251977  6.65426244]\n",
            " [-0.70628076  3.28919234]\n",
            " [ 0.3575458   1.74498934]\n",
            " [ 0.28000093 -0.58458918]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Backward propagation"
      ],
      "metadata": {
        "id": "kznUlguyperp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = [1, -4, 9] # specify dL/dz\n",
        "loss_t = torch.tensor(loss) # convert it to tensor\n",
        "LOSS = torch.tensor(np.random.rand(4, 2)) # specify dL/dz for matrix multiplication"
      ],
      "metadata": {
        "id": "HR41Je1RHijP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b = SoftMax_backward_prop(a, loss) # applying SoftMax back propagation using my function\n",
        "res1 = nn.functional.softmax(a_t1, dim=0).backward(loss_t) # applying SoftMax back propagation using pytorch\n",
        "# comparing results\n",
        "print('Mine softmax backward output:', b)\n",
        "print('Torch softmax backward output:', a_t1.grad.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoozYdLhFa_E",
        "outputId": "4966b1f4-6c14-4787-e837-140c2c4f474f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mine softmax backward output: [-0.74687172 -0.48480908  1.23168081]\n",
            "Torch softmax backward output: [-0.7468716502189636, -0.48480910062789917, 1.2316807508468628]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b = RelU_backward_prop(a, loss) # applying RelU back propagation using my function\n",
        "res2 = nn.functional.relu(a_t2).backward(loss_t) # applying RelU back propagation using pytorch\n",
        "# comparing results\n",
        "print('Mine RelU backward output:', b)\n",
        "print('Torch RelU backward output:', a_t2.grad.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yf1VvEYY90C7",
        "outputId": "5c9c60c4-3217-410a-8a3f-4426f252796b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mine RelU backward output: [ 1. -4.  9.]\n",
            "Torch RelU backward output: [1.0, -4.0, 9.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b = MatMul_backward_prop(LOSS.numpy(), W) # applying matrix multiplication back propagation using my function\n",
        "res2 = nn.functional.linear(X_t.t(), weight=W_t).backward(LOSS.t()) # applying matrix multiplication back propagation using pytorch\n",
        "# X_t is transposed, because the difference between the order of multiplication in my function and in pytorch\n",
        "# output is transposed as well\n",
        "\n",
        "# comparing results\n",
        "print('Mine matrix multiplication backward output:\\n', b)\n",
        "print('Torch matrix multiplication backward output:\\n', X_t.grad.t().numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UAHWEyaCupD",
        "outputId": "2e048628-cdf3-40c6-8195-0af47f223332"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mine matrix multiplication backward output:\n",
            " [[0.81823771 1.59993299 0.73531026]\n",
            " [0.81280535 0.99103996 0.53722786]]\n",
            "Torch matrix multiplication backward output:\n",
            " [[0.81823771 1.59993299 0.73531026]\n",
            " [0.81280535 0.99103996 0.53722786]]\n"
          ]
        }
      ]
    }
  ]
}