{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "GfVKkm94KBfR",
        "BWemJAqvJhem",
        "jPLv8BwuJ27M",
        "5k5dKRqdcFvL",
        "RB0Kpd03IkGy",
        "ptzO13bgfp4K"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEhmtS9kDy60"
      },
      "outputs": [],
      "source": [
        "#importing everything required for the lab\n",
        "import numpy as np\n",
        "import time\n",
        "from tensorflow import keras\n",
        "from tqdm import tqdm\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Copying functions from previous labs"
      ],
      "metadata": {
        "id": "BKkTDKTjJYoJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Compute padding"
      ],
      "metadata": {
        "id": "GfVKkm94KBfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def compute_pad_filt(input_size, output_size):\n",
        "  Hx, Wx = input_size\n",
        "  Hy, Wy = output_size\n",
        "\n",
        "  min_pad_h, min_pad_w = 1, 1 # minimal padding is (1, 1)\n",
        "\n",
        "  min_h, min_w = Hx-Hy+3, Wx-Wy+3 # for minimal padding kernel size is: min_h = Hx - Hy + 2 * pad[0] + 1, min_w = Wx - Wy + 2 * pad[1] + 1\n",
        "\n",
        "  if min_h < 1: # minimal size for height of a kernel = 1\n",
        "    min_pad_h = 1 + math.ceil((1-min_h) / 2) # add paddings so kernel height greater or equal to 1\n",
        "    min_h = min_h + (min_pad_h-1) * 2 # calculate new height\n",
        "\n",
        "\n",
        "  if min_w < 1: # minimal size for width of a kernel = 1\n",
        "    min_pad_w = 1 + math.ceil((1-min_w) / 2) # add paddings so kernel width greater or equal to 1\n",
        "    min_w = min_w + (min_pad_w-1) * 2  # calculate new height\n",
        "\n",
        "\n",
        "  return (min_pad_h, min_pad_w), (min_h, min_w)"
      ],
      "metadata": {
        "id": "rq7YIfJyKBF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolution"
      ],
      "metadata": {
        "id": "BWemJAqvJhem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# forward propagation for convolution\n",
        "# add padings\n",
        "def conv2d_forward(matrix, filter, pad=(0,0)):\n",
        "  if pad != (0,0): # if there are paddings, apply them\n",
        "    matrix = np.pad(matrix, ((pad[0], pad[0]),(pad[1], pad[1]),(0,0)))\n",
        "  h_x, w_x, _ = matrix.shape # getting matrix shape\n",
        "  h_w, w_w, _  = filter.shape # getting filter shape\n",
        "  output = np.zeros((h_x - h_w + 1, w_x - w_w + 1)) # initializing output matrix\n",
        "  for i in range(len(output)): # for each pixel\n",
        "    for j in range(len(output[i])):\n",
        "        output[i][j] = np.sum(matrix[i:i+h_w, j:j+w_w, :] * filter) # calculate sum of hadamart product between\n",
        "                                                                    # matrix batch and filter, save value in output cell\n",
        "  return output"
      ],
      "metadata": {
        "id": "4jkHBWEmJXaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# backward propagation for convolution (dL/dZ)\n",
        "def conv2d_backward_pad(upstream, filter, pad=(0,0)):\n",
        "  # if there are paddings, we send them to conv2d_forward\n",
        "  h_w, w_w, d_w  = filter.shape # getting filter shape\n",
        "  rotated_filter = np.rot90(np.rot90(filter)) # rotate filter by 180 degree\n",
        "  dL_dZ = [] # initializing output\n",
        "  for i in range(d_w): # for each channel\n",
        "    dL_dZ.append(conv2d_forward(upstream, rotated_filter[:, :, i, np.newaxis], pad)) # adding dL/dZ\n",
        "  return np.array(dL_dZ)"
      ],
      "metadata": {
        "id": "Wt-vYIhvJlSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# backward propagation for convolution (dL/dW)\n",
        "def conv2d_backward_weights(weights, upstream, pad=(0,0)):\n",
        "   # if there are paddings, apply them\n",
        "  if pad != (0,0):\n",
        "    weights = np.pad(weights, ((pad[0], pad[0]),(pad[1], pad[1]),(0,0)))\n",
        "  h_x, w_x, d_x  = weights.shape # getting filter shape\n",
        "  dL_dZ = [] # initializing output\n",
        "  for i in range(d_x): # for each channel\n",
        "    dL_dZ.append(conv2d_forward(weights[:, :, i, np.newaxis], upstream)) # adding dL/dZ\n",
        "  return np.transpose(np.array(dL_dZ), (1, 2, 0))"
      ],
      "metadata": {
        "id": "NlsBN9tqJnBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ReLU"
      ],
      "metadata": {
        "id": "CuF8nrrlJnd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function for RelU forward and backward propagation\n",
        "#taken from previous assignments\n",
        "def RelU_jacobian(input):\n",
        "  return 1 * (input > 0)\n",
        "\n",
        "def RelU_forward_prop(input):\n",
        "  return np.maximum(input, 0)\n",
        "\n",
        "def RelU_backward_prop(input, loss):\n",
        "  jac = RelU_jacobian(input) # finding jacobian for RelU according to input\n",
        "  return jac * np.array(loss)"
      ],
      "metadata": {
        "id": "H3Cz0h1LJ69I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matmul"
      ],
      "metadata": {
        "id": "jPLv8BwuJ27M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#functions for matmul backward and forward propagation from previous assignments\n",
        "def MatMul_forward_prop(matrix, input):\n",
        "  return np.array(matrix) @ np.array(input)\n",
        "\n",
        "#function that finds dL/dx\n",
        "def MatMul_backward_prop(matrix, loss):\n",
        "  return np.array(matrix).T @ np.array(loss)\n",
        "\n",
        "#function that finds dL/dW\n",
        "def MatMul_matrix_backward_prop(X, loss):\n",
        "  return np.array(loss) @ np.array(X).T"
      ],
      "metadata": {
        "id": "1exvfK7jOtzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Labels vectorization"
      ],
      "metadata": {
        "id": "5k5dKRqdcFvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function taken from previous assignments\n",
        "#it translates label number into the vector of 0s and 1s\n",
        "def label_vec_func(labels):\n",
        "  labels_matrix = np.zeros([len(labels), c])\n",
        "  for i in range(len(labels)):\n",
        "    labels_matrix[i, labels[i]] = 1\n",
        "  return labels_matrix"
      ],
      "metadata": {
        "id": "565Ye-qfROCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolution for many filters"
      ],
      "metadata": {
        "id": "RB0Kpd03IkGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# forward propagation for convolution\n",
        "# now we have filters - array of size: (height, width, depth, number of filters)\n",
        "# add paddings to the function calls\n",
        "def conv2d_forward_many(matrix, filters, pad=(0,0)):\n",
        "  h_x, w_x, _ = matrix.shape # getting matrix shape\n",
        "  h_w, w_w, _, d  = filters.shape # getting filter shape\n",
        "  output = np.zeros((h_x - h_w + 1 + 2 *pad[0], w_x - w_w + 1 + 2 * pad[1], d)) # initializing output matrix\n",
        "  for k in range(d): #for each filter\n",
        "    output[:, :, k] = conv2d_forward(matrix, filters[:, :, :, k], pad)\n",
        "  return output"
      ],
      "metadata": {
        "id": "dZEgEccRcBHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# backward propagation for convolution (dL/dZ)\n",
        "# now we have filters - array of size: (height, width, depth, number of filters)\n",
        "# add paddings to the function calls\n",
        "def conv2d_backward_many(upstream, filters, pad=(0,0)):\n",
        "  h_w, w_w, d_w, D  = filters.shape # getting filter shape\n",
        "  dL_dZ = []\n",
        "  for i in range(D):  # for each filter\n",
        "    dL_dZ.append(conv2d_backward_pad(upstream[:, :, i, np.newaxis], filters[:, :, : , i], pad))\n",
        "  return np.transpose(np.sum(dL_dZ, 0), (1, 2, 0))"
      ],
      "metadata": {
        "id": "_2_VDSVeqhPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# backward propagation for convolution (dL/dW)\n",
        "# now we have filters - array of size: (height, width, depth, number of filters)\n",
        "# add paddings to the function calls\n",
        "def conv2d_backward_weights_many(weight, upstream, pad=(0,0)):\n",
        "  _, _, D  = upstream.shape # getting filter shape\n",
        "  dL_dWs = [] # initializing output\n",
        "  for i in range(D): # for each channel\n",
        "    dL_dWs.append(conv2d_backward_weights(weight, upstream[:, :, i, np.newaxis], pad))\n",
        "  return np.transpose(np.array(dL_dWs), (1, 2, 3, 0))"
      ],
      "metadata": {
        "id": "GEKpsqIdqlvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resize"
      ],
      "metadata": {
        "id": "ptzO13bgfp4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for rescaling I used Nearest-neighbour interpolation\n",
        "def nearest(input, width, height):\n",
        "  \"\"\"Function that takes photo, rescales it to specified width and height, given as a parameters\n",
        "\n",
        "    Reference to the source that I used: https://kwojcicki.github.io/blog/NEAREST-NEIGHBOUR\n",
        "\n",
        "    Keyword arguments:\n",
        "      input (np.array): input image represented as numpy array\n",
        "      width (int) -- width of a new rescaled image\n",
        "      height (int) -- height of a new rescaled image\n",
        "\n",
        "    Returns:\n",
        "      output (np.array): output (rescaled) image represented as numpy array\n",
        "  \"\"\"\n",
        "  # initialization of output variable\n",
        "  output = np.zeros((width, height, 3), dtype=int)\n",
        "  # calculation of scales between input's width, height and\n",
        "  sx = input.shape[0] / output.shape[0]\n",
        "  sy = input.shape[1] / output.shape[1]\n",
        "  # for each pixel from output image\n",
        "  for y in range(len(output)):\n",
        "    for x in range(len(output[y])):\n",
        "      # finding nearest input's pixel for current output's pixel\n",
        "      proj_x = math.floor(x * sx)\n",
        "      proj_y = math.floor(y * sy)\n",
        "\n",
        "      # initialize output's pixel using obtained nearest input's pixel\n",
        "      output[y][x] = input[proj_y][proj_x]\n",
        "\n",
        "  return output"
      ],
      "metadata": {
        "id": "jZdk6_mwfpRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Squared Error"
      ],
      "metadata": {
        "id": "_c9vGHcEb6GJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SE_forward(y_pred, y_true):\n",
        "  output = y_pred - y_true\n",
        "  return np.sum(output ** 2)"
      ],
      "metadata": {
        "id": "JLnfZov0Iw4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def SE_backward(y_pred, y_true):\n",
        "  output = y_pred - y_true\n",
        "  return 2 * output"
      ],
      "metadata": {
        "id": "gUZq3vOMJvtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Labels vectorization"
      ],
      "metadata": {
        "id": "yaOo-ZLNncPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function taken from previous assignments\n",
        "#it translates label number into the vector of 0s and 1s\n",
        "def label_vec_func(labels, c):\n",
        "  labels_matrix = np.zeros([len(labels), c])\n",
        "  for i in range(len(labels)):\n",
        "    labels_matrix[i, labels[i]] = 1\n",
        "  return labels_matrix"
      ],
      "metadata": {
        "id": "NuijL5MJnckJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SoftMax"
      ],
      "metadata": {
        "id": "EeomTjUi3wzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function for softmax forward and backward propagation\n",
        "#taken from previous assignments\n",
        "def SoftMax_forward_prop(input, normalization=False):\n",
        "  output = np.array(input, dtype=np.longdouble)\n",
        "  if normalization: # if we use normalization\n",
        "    output = output - np.max(input) # we substract maximal value from each number\n",
        "  output = np.exp(output)\n",
        "  return output / np.sum(output)\n",
        "\n",
        "def SoftMax_jacobian(input, normalization=False): # function for calculating jacobian of SoftMax according to input\n",
        "  output = SoftMax_forward_prop(input, normalization)\n",
        "  jacobian = np.zeros((len(input), len(input)))\n",
        "  for i in range(len(input)):\n",
        "    for j in range(len(input)):\n",
        "      if i == j:\n",
        "        jacobian[i][j] = output[i] * (1 - output[j])\n",
        "      else:\n",
        "        jacobian[i][j] = -output[i] * output[j]\n",
        "  return jacobian\n",
        "\n",
        "def SoftMax_backward_prop(input, loss, normalization=False): # backpropagation\n",
        "  jac = SoftMax_jacobian(input, normalization) # calculating jacobian\n",
        "  return jac @ np.array(loss)"
      ],
      "metadata": {
        "id": "65v2BqQk3u76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Log softmax"
      ],
      "metadata": {
        "id": "BH7J5sBo3dfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#functions for log_softmax forward and backward propagation\n",
        "#this node applies softmax and then finds logorithm of the result\n",
        "def log_softmax(x):\n",
        "  x_max = np.max(x)\n",
        "  return x - x_max - np.log(np.sum(np.exp(x - x_max)))\n",
        "\n",
        "def log_softmax_jacobian(input): # function for calculating jacobian of SoftMax according to input\n",
        "  output = SoftMax_forward_prop(input, True)\n",
        "  jacobian = np.zeros((len(input), len(input)))\n",
        "  for i in range(len(input)):\n",
        "    for j in range(len(input)):\n",
        "      if i == j:\n",
        "        jacobian[i][j] = (1 - output[j])\n",
        "      else:\n",
        "        jacobian[i][j] = -output[j]\n",
        "  return jacobian\n",
        "\n",
        "def log_softmax_backward_prop(input, loss): # backpropagation\n",
        "  jac = log_softmax_jacobian(input) # calculating jacobian\n",
        "  return jac @ np.array(loss)"
      ],
      "metadata": {
        "id": "IvkRN8Xo3bhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading and preprocessing dataset"
      ],
      "metadata": {
        "id": "MOnFAhPQXyjG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading [dataset](https://app.roboflow.com/fourcolorsgame-gmail-com/cockroachesvsbushes/1)"
      ],
      "metadata": {
        "id": "d4l86NjSt0-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"iwiRB6xLJgD2LBn0pWPH\")\n",
        "project = rf.workspace(\"fourcolorsgame-gmail-com\").project(\"cockroachesvsbushes\")\n",
        "dataset = project.version(1).download(\"coco\")\n"
      ],
      "metadata": {
        "id": "y-02QqeDLZBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to read and preprocess dataset from downloaded folder"
      ],
      "metadata": {
        "id": "drLB-bATtdb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "def read_dataset(directory, h, w):\n",
        "  annotation =  eval(open(directory + '/_annotations.coco.json').read())\n",
        "  x = []\n",
        "  boxes = []\n",
        "  classes = []\n",
        "  for data, image in zip(annotation['annotations'], annotation['images']):\n",
        "    path = image['file_name']\n",
        "    box  = data['bbox']\n",
        "    classes.append(data['category_id'])\n",
        "    box[2], box[3] = box[0] + box[2], box[1] + box[3] # convert from [x,y,width,height] to [x1,y1,x2,y2] notation\n",
        "    boxes.append(np.array(box))\n",
        "    image = cv2.imread(directory + '/' + path)\n",
        "    # image = nearest(image, h, w)\n",
        "    x.append(image)\n",
        "  return np.array(x), np.array(boxes), np.array(classes)"
      ],
      "metadata": {
        "id": "p4cBrTJRZRe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h, w = 64, 64 # initial height and width of pictures are 64, 64\n",
        "n = h * w #number of pixels for one picture"
      ],
      "metadata": {
        "id": "4AsA5JUEn63x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train, y_classes_train = read_dataset('/content/CockroachesVsBushes-1/train', h, w) # read nad preprocess train dataset\n",
        "x_test, y_test, y_classes_test = read_dataset('/content/CockroachesVsBushes-1/valid', h, w) # read nad preprocess test dataset"
      ],
      "metadata": {
        "id": "coiYF8jGfZ2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c = 4 # number of outputs for last linear layer for boxes\n",
        "k = 3 # number of classes"
      ],
      "metadata": {
        "id": "8JJH4IAKKagu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_classes_train = label_vec_func(y_classes_train - 1, k) #converting labels to vector of 0s and 1s"
      ],
      "metadata": {
        "id": "2jJUbcrZ4mIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_train, N_test = len(x_train), len(x_test) # calculating number of samples in train and test dataset"
      ],
      "metadata": {
        "id": "evm0c63ud3IC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "tKFGshuEKfVH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scheme 2 (from lecture 12, slide 9)"
      ],
      "metadata": {
        "id": "NnFAo4oJF4b_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Localization part"
      ],
      "metadata": {
        "id": "LNmmPJI6F53w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Defining localization model"
      ],
      "metadata": {
        "id": "NUasOc2rr-za"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pad, kernel = compute_pad_filt((h, w), (h, w)) # we want output to have the smae dimensions\n",
        "\n",
        "hw1, Ww1  = kernel # initialize kernel sizes for first layer\n",
        "d, D1 = 3, 2 # initialize depth and number of filters for first layer\n",
        "\n",
        "hw2, Ww2, D2 = hw1, Ww1, 1 # initialize kernel sizes, number of filters for second layer\n",
        "\n",
        "Conv1_loc =  np.random.uniform(-1, 1, (hw1, Ww1, d, D1)) # initial convolution filters for layer 1\n",
        "b1_loc = np.random.uniform(-1, 1 , (h, w, D1)) #b1 - initial bias\n",
        "Conv2_loc =  np.random.uniform(-1, 1, (hw2, Ww2, D1, D2))# initial convolution filters for layer 2\n",
        "b2_loc = np.random.uniform(-1, 1 , (h, w, D2)) #b2 - initial bias\n",
        "\n",
        "vectorized_len = h * w * D2 # size for vectorized tensor after 2 convulitions\n",
        "\n",
        "c1 = vectorized_len // 4 # dimension of the first layer\n",
        "W1_loc = np.random.uniform(-1, 1, (c1, vectorized_len)) #W1 - initial weights\n",
        "W2_loc = np.random.uniform(-1, 1, (c, c1)) #W2 - initial weights\n",
        "b3_loc = np.random.uniform(-1, 1 , (c1, 1)) #b1 - initial bias\n",
        "b4_loc = np.random.uniform(-1, 1 , (c, 1)) #b2 - initial bias\n",
        "\n",
        "\n",
        "nu = 0.001 # learning rate\n",
        "num_epochs = 20 # amount of epochs\n",
        "\n",
        "N = 2 # number of images in minibatch\n",
        "\n",
        "# initialize partial derivatives\n",
        "dL_dConv1_loc = np.zeros((hw1, Ww1, d, D1))\n",
        "dL_dConv2_loc = np.zeros((hw2, Ww2, D1, D2))\n",
        "dL_dW1_loc = np.zeros((c1, vectorized_len))\n",
        "dL_dW2_loc = np.zeros((c, c1))\n",
        "dL_db1_loc = 0\n",
        "dL_db2_loc = 0\n",
        "dL_db3_loc = 0\n",
        "dL_db4_loc = 0"
      ],
      "metadata": {
        "id": "GeR-P7ZqO7L2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training process"
      ],
      "metadata": {
        "id": "mkEY5yessDeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(num_epochs): #for each epoch\n",
        "  total_loc_loss = 0 #sum of losses of localization for one epoch\n",
        "  counter = 0 #counter to check that batch ended\n",
        "  for i in tqdm(range(N_train)): #for each picture\n",
        "    # if y_classes_train[i][2] == 1: # skip if it is a 'none' class\n",
        "    #   continue\n",
        "\n",
        "    x = x_train[i, :, :] / 255 #normalize pixels, so they will be from 0 to 1\n",
        "\n",
        "    y_true_loc = y_train[i].reshape(c, 1) / 64\n",
        "    #forward propagation\n",
        "\n",
        "    y1 = conv2d_forward_many(x, Conv1_loc, pad=pad) #applying convoltion filters from layer 1\n",
        "    y2 = y1 + b1_loc #adding bias\n",
        "    y3 = RelU_forward_prop(y2) #applying RelU\n",
        "    y4 =  conv2d_forward_many(y3, Conv2_loc, pad=pad) #applying convoltion filters from layer 2\n",
        "    y5 = y4 + b2_loc #adding bias\n",
        "    y6 = RelU_forward_prop(y5) #applying RelU\n",
        "\n",
        "    y7 = np.reshape(y6, (vectorized_len, 1))\n",
        "\n",
        "    # Segmentation\n",
        "    y8_loc = MatMul_forward_prop(W1_loc, y7) #applying matrix multiplication with the first weight matrix\n",
        "    y9_loc = y8_loc + b3_loc #adding bias\n",
        "    y10_loc = RelU_forward_prop(y9_loc) #applying RelU\n",
        "    y11_loc =  MatMul_forward_prop(W2_loc, y10_loc) #applying matrix multiplication with the second weight matrix\n",
        "    y12_loc = y11_loc + b4_loc #adding bias\n",
        "    # no ReLU after bias addition, since we have nonlinearity further (softmax)\n",
        "    loc_loss = SE_forward(y12_loc, y_true_loc)\n",
        "\n",
        "\n",
        "    total_loc_loss += loc_loss #adding current loss to total loss\n",
        "\n",
        "    #backward propagation over segmentation\n",
        "\n",
        "    back_loc = SE_backward(y12_loc, y_true_loc) #backpropagation from loss to the input of softmax\n",
        "\n",
        "    dL_db4_loc += back_loc #backpropagation from loss to the input of addition, finding dL/db4\n",
        "\n",
        "    dL_dW2_loc += MatMul_matrix_backward_prop(y10_loc, back_loc) #fiding dL/dW2\n",
        "\n",
        "    back_loc = MatMul_backward_prop(W2_loc, back_loc) #backpropagation from loss to the input of matrix multiplication with matrix W2\n",
        "\n",
        "    back_loc = RelU_backward_prop(y9_loc, back_loc) #backpropagation from loss to the input of RelU\n",
        "\n",
        "    dL_db3_loc += back_loc #backpropagation from loss to the input of addition, finding dL/db3\n",
        "\n",
        "    dL_dW1_loc += MatMul_matrix_backward_prop(y7, back_loc) #fiding dL/dW1\n",
        "\n",
        "    back_loc = MatMul_backward_prop(W1_loc, back_loc) #backpropagation from addition to the input of matrix multiplication with matrix W1\n",
        "\n",
        "    back_loc = back_loc.reshape((h, w, D2)) # backpropagation of reshaping\n",
        "\n",
        "    back = back_loc\n",
        "\n",
        "    back = RelU_backward_prop(y5, back) # backpropagation from reshaping to the input of RelU\n",
        "\n",
        "    dL_db2_loc += back #backpropagation from ReLU to the input of addition, finding dL/db2\n",
        "\n",
        "    dL_dConv2_loc += conv2d_backward_weights_many(y3, back, pad=pad) #finding dL/dConv2\n",
        "\n",
        "    back = conv2d_backward_many(back, Conv2_loc, pad=pad) # backpropagation from addition to the input convolution\n",
        "    back = RelU_backward_prop(y2, back) # backpropagation from convolution to the input of RelU\n",
        "\n",
        "    dL_db1_loc += back #backpropagation from ReLU to the input of addition, finding dL/db1\n",
        "    dL_dConv1_loc += conv2d_backward_weights_many(x, back, pad=(1,1)) #finding dL/dConv1\n",
        "\n",
        "    counter += 1 # increasing counter\n",
        "\n",
        "    if counter == N or i == N_train - 1: # if batch ended or dataset ended. We apply gradient descent only in batches\n",
        "    #applying gradient descent for weights and biases\n",
        "\n",
        "      Conv1_loc = Conv1_loc - nu / N * dL_dConv1_loc\n",
        "      Conv2_loc = Conv2_loc - nu / N * dL_dConv2_loc\n",
        "\n",
        "      W1_loc = W1_loc - nu / N * dL_dW1_loc\n",
        "      W2_loc = W2_loc - nu / N * dL_dW2_loc\n",
        "\n",
        "\n",
        "      b1_loc = b1_loc - nu / N * dL_db1_loc\n",
        "      b2_loc = b2_loc - nu / N * dL_db2_loc\n",
        "\n",
        "      b3_loc = b3_loc - nu / N * dL_db3_loc\n",
        "      b4_loc = b4_loc - nu / N * dL_db4_loc\n",
        "\n",
        "\n",
        "      # setting partial derivatives to 0\n",
        "      dL_dConv1_loc = np.zeros((hw1, Ww1, d, D1))\n",
        "      dL_dConv2_loc = np.zeros((hw2, Ww2, D1, D2))\n",
        "\n",
        "      dL_dW1_loc = np.zeros((c1, vectorized_len))\n",
        "      dL_dW2_loc = np.zeros((c, c1))\n",
        "\n",
        "\n",
        "      dL_db1_loc = 0\n",
        "      dL_db2_loc = 0\n",
        "\n",
        "      dL_db3_loc = 0\n",
        "      dL_db4_loc = 0\n",
        "\n",
        "\n",
        "      counter = 0\n",
        "  print('\\nSegmentation Loss:', total_loc_loss / N_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2I5gCn_FwFs",
        "outputId": "aaa5f4bd-2373-4c19-8a79-592c898f7e7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [00:31<00:00,  2.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Segmentation Loss: 153812.3568583004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [00:28<00:00,  2.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Segmentation Loss: 0.8142951589539035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [00:28<00:00,  2.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Segmentation Loss: 0.6959230166060532\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [00:28<00:00,  2.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Segmentation Loss: 0.5995813418137018\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [00:28<00:00,  2.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Segmentation Loss: 0.520833482027385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [00:28<00:00,  2.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Segmentation Loss: 0.4562675502081685\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [00:28<00:00,  2.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Segmentation Loss: 0.40321683346786685\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [00:28<00:00,  2.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Segmentation Loss: 0.3589370020319063\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [00:27<00:00,  2.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Segmentation Loss: 0.3240311049343034\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [00:27<00:00,  2.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Segmentation Loss: 0.2959570734945965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [00:30<00:00,  2.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Segmentation Loss: 0.27253646371884205\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [00:28<00:00,  2.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Segmentation Loss: 0.2530009238140344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [00:28<00:00,  2.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Segmentation Loss: 0.23670952853649613\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [00:28<00:00,  2.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Segmentation Loss: 0.22312739214440352\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [00:28<00:00,  2.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Segmentation Loss: 0.2118076265961144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [00:28<00:00,  2.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Segmentation Loss: 0.20237664645583808\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [00:28<00:00,  2.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Segmentation Loss: 0.19452207591086107\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [00:27<00:00,  2.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Segmentation Loss: 0.18798272699808913\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [00:27<00:00,  2.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Segmentation Loss: 0.18254025481394384\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [00:34<00:00,  2.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Segmentation Loss: 0.17801218718568007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classificatioon"
      ],
      "metadata": {
        "id": "BUgIQ5I5FxyT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Defining classification model"
      ],
      "metadata": {
        "id": "jki5G2_usHUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pad, kernel = compute_pad_filt((h, w), (h, w)) # we want output to have the smae dimensions\n",
        "\n",
        "hw1, Ww1  = kernel # initialize kernel sizes for first layer\n",
        "d, D1 = 3, 2 # initialize depth and number of filters for first layer\n",
        "\n",
        "hw2, Ww2, D2 = hw1, Ww1, 2 # initialize kernel sizes, number of filters for second layer\n",
        "\n",
        "Conv1_cl =  np.random.uniform(-1, 1, (hw1, Ww1, d, D1)) # initial convolution filters for layer 1\n",
        "b1_cl = np.random.uniform(-1, 1 , (h, w, D1)) #b1 - initial bias\n",
        "Conv2_cl =  np.random.uniform(-1, 1, (hw2, Ww2, D1, D2))# initial convolution filters for layer 2\n",
        "b2_cl = np.random.uniform(-1, 1 , (h, w, D2)) #b2 - initial bias\n",
        "\n",
        "vectorized_len = h * w * D2 # size for vectorized tensor after 2 convulitions\n",
        "\n",
        "c1 = vectorized_len // 4 # dimension of the first layer\n",
        "k1 = c1\n",
        "\n",
        "W1_cl = np.random.uniform(-1, 1, (k1, vectorized_len)) #W1 - initial weights\n",
        "W2_cl = np.random.uniform(-1, 1, (k, k1)) #W2 - initial weights\n",
        "b3_cl = np.random.uniform(-1, 1 , (k1, 1)) #b1 - initial bias\n",
        "b4_cl = np.random.uniform(-1, 1 , (k, 1)) #b2 - initial bias\n",
        "\n",
        "\n",
        "nu = 0.001 # learning rate\n",
        "num_epochs = 20 # amount of epochs\n",
        "\n",
        "N = 2 # number of images in minibatch\n",
        "\n",
        "# initialize partial derivatives\n",
        "dL_dConv1_cl = np.zeros((hw1, Ww1, d, D1))\n",
        "dL_dConv2_cl = np.zeros((hw2, Ww2, D1, D2))\n",
        "\n",
        "\n",
        "dL_dW1_cl = np.zeros((k1, vectorized_len))\n",
        "dL_dW2_cl = np.zeros((k, k1))\n",
        "\n",
        "dL_db1_cl = 0\n",
        "dL_db2_cl = 0\n",
        "\n",
        "\n",
        "dL_db3_cl = 0\n",
        "dL_db4_cl = 0"
      ],
      "metadata": {
        "id": "miVY8SraiOxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training process"
      ],
      "metadata": {
        "id": "Qk9fSlUesKVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(num_epochs): #for each epoch\n",
        "  total_cl_loss = 0 #sum of losses of classification for one epoch\n",
        "  counter = 0 #counter to check that batch ended\n",
        "  for i in tqdm(range(N_train)): #for each picture\n",
        "\n",
        "    x = x_train[i, :, :] / 255 #normalize pixels, so they will be from 0 to 1\n",
        "\n",
        "    y_true_cl = np.array(y_classes_train[i].reshape(k,  1))\n",
        "    #forward propagation\n",
        "\n",
        "    y1 = conv2d_forward_many(x, Conv1_cl, pad=pad) #applying convoltion filters from layer 1\n",
        "    y2 = y1 + b1_cl #adding bias\n",
        "    y3 = RelU_forward_prop(y2) #applying RelU\n",
        "    y4 =  conv2d_forward_many(y3, Conv2_cl, pad=pad) #applying convoltion filters from layer 2\n",
        "    y5 = y4 + b2_cl #adding bias\n",
        "    y6 = RelU_forward_prop(y5) #applying RelU\n",
        "\n",
        "    y7 = np.reshape(y6, (vectorized_len, 1))\n",
        "\n",
        "    # Classification\n",
        "    y8_cl = MatMul_forward_prop(W1_cl, y7) #applying matrix multiplication with the first weight matrix\n",
        "    y9_cl = y8_cl + b3_cl #adding bias\n",
        "    y10_cl = RelU_forward_prop(y9_cl) #applying RelU\n",
        "    y11_cl =  MatMul_forward_prop(W2_cl, y10_cl) #applying matrix multiplication with the second weight matrix\n",
        "    y12_cl = y11_cl + b4_cl #adding bias\n",
        "    y13_cl = log_softmax(y12_cl) #applying log softmax\n",
        "    # no ReLU after bias addition, since we have nonlinearity further (softmax)\n",
        "    cl_loss = y_true_cl.T @ y13_cl #finding loss\n",
        "\n",
        "\n",
        "    total_cl_loss += -cl_loss.item() #adding current loss to total loss\n",
        "\n",
        "    #backward propagation over calssification\n",
        "\n",
        "    back_cl = -y_true_cl + SoftMax_forward_prop(y12_cl, True) #backpropagation from loss to the input of softmax\n",
        "\n",
        "    dL_db4_cl += back_cl #backpropagation from loss to the input of addition, finding dL/db4\n",
        "\n",
        "    dL_dW2_cl += MatMul_matrix_backward_prop(y10_cl, back_cl) #fiding dL/dW2\n",
        "\n",
        "    back_cl = MatMul_backward_prop(W2_cl, back_cl) #backpropagation from loss to the input of matrix multiplication with matrix W2\n",
        "\n",
        "    back_cl = RelU_backward_prop(y9_cl, back_cl) #backpropagation from loss to the input of RelU\n",
        "\n",
        "    dL_db3_cl += back_cl #backpropagation from loss to the input of addition, finding dL/db3\n",
        "\n",
        "    dL_dW1_cl += MatMul_matrix_backward_prop(y7, back_cl) #fiding dL/dW1\n",
        "\n",
        "    back_cl = MatMul_backward_prop(W1_cl, back_cl) #backpropagation from addition to the input of matrix multiplication with matrix W1\n",
        "\n",
        "    back_cl = back_cl.reshape((h, w, D2)) # backpropagation of reshaping\n",
        "\n",
        "    back = back_cl\n",
        "\n",
        "    back = RelU_backward_prop(y5, back) # backpropagation from reshaping to the input of RelU\n",
        "\n",
        "    dL_db2_cl += back #backpropagation from ReLU to the input of addition, finding dL/db2\n",
        "\n",
        "    dL_dConv2_cl += conv2d_backward_weights_many(y3, back, pad=pad) #finding dL/dConv2\n",
        "\n",
        "    back = conv2d_backward_many(back, Conv2_cl, pad=pad) # backpropagation from addition to the input convolution\n",
        "    back = RelU_backward_prop(y2, back) # backpropagation from convolution to the input of RelU\n",
        "\n",
        "    dL_db1_cl += back #backpropagation from ReLU to the input of addition, finding dL/db1\n",
        "    dL_dConv1_cl += conv2d_backward_weights_many(x, back, pad=(1,1)) #finding dL/dConv1\n",
        "\n",
        "    counter += 1 # increasing counter\n",
        "\n",
        "    if counter == N or i == N_train - 1: # if batch ended or dataset ended. We apply gradient descent only in batches\n",
        "    #applying gradient descent for weights and biases\n",
        "\n",
        "      Conv1_cl = Conv1_cl - nu / N * dL_dConv1_cl\n",
        "      Conv2_cl = Conv2_cl - nu / N * dL_dConv2_cl\n",
        "\n",
        "\n",
        "      W1_cl = W1_cl - nu / N * dL_dW1_cl\n",
        "      W2_cl = W2_cl - nu / N * dL_dW2_cl\n",
        "\n",
        "      b1_cl = b1_cl - nu / N * dL_db1_cl\n",
        "      b2_cl = b2_cl - nu / N * dL_db2_cl\n",
        "\n",
        "\n",
        "      b3_cl = b3_cl - nu / N * dL_db3_cl\n",
        "      b4_cl = b4_cl - nu / N * dL_db4_cl\n",
        "\n",
        "      # setting partial derivatives to 0\n",
        "      dL_dConv1_cl = np.zeros((hw1, Ww1, d, D1))\n",
        "      dL_dConv2_cl = np.zeros((hw2, Ww2, D1, D2))\n",
        "\n",
        "\n",
        "      dL_dW1_cl = np.zeros((k1, vectorized_len))\n",
        "      dL_dW2_cl = np.zeros((k, k1))\n",
        "\n",
        "      dL_db1_cl = 0\n",
        "      dL_db2_cl = 0\n",
        "\n",
        "\n",
        "      dL_db3_cl = 0\n",
        "      dL_db4_cl = 0\n",
        "\n",
        "      counter = 0\n",
        "  print('\\nClassification Loss:', total_cl_loss / N_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEa1p_u_FwjB",
        "outputId": "bf628630-3abf-4eb5-b1f2-78bbf23c2e3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [02:03<00:00,  1.54s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Loss: 53.93980977544809937\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [01:58<00:00,  1.48s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Loss: 3.8029473681674140883\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [01:58<00:00,  1.48s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Loss: 2.2444955191707742258\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [02:00<00:00,  1.51s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Loss: 1.6391668648615694483\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [01:58<00:00,  1.48s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Loss: 1.2088372106614907326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [01:58<00:00,  1.48s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Loss: 0.8202005505109583461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [02:00<00:00,  1.50s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Loss: 0.7011958847908114399\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [01:58<00:00,  1.48s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Loss: 0.6247908421174317648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [02:00<00:00,  1.50s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Loss: 0.58067801295048149924\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [01:59<00:00,  1.49s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Loss: 0.5475409978445247157\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [01:58<00:00,  1.48s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Loss: 0.51869862547589010176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [02:00<00:00,  1.51s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Loss: 0.52228665173767035084\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [01:58<00:00,  1.49s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Loss: 0.48732316769358207512\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [02:00<00:00,  1.51s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Loss: 0.47731770120562610877\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [01:58<00:00,  1.49s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Loss: 0.46947975644692576156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [01:58<00:00,  1.48s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Loss: 0.6247400328700462134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [02:01<00:00,  1.52s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Loss: 0.45871415396062884245\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [01:58<00:00,  1.49s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Loss: 0.42412431966032972485\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [02:01<00:00,  1.51s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Loss: 0.41494146877228135846\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 80/80 [01:59<00:00,  1.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Loss: 0.40115462276645235475\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "jz2mlHV46Vth"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification model"
      ],
      "metadata": {
        "id": "-tfwLzd4cfHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = np.zeros((N_test, 1), int) #predictions\n",
        "for i in tqdm(range(N_test)):\n",
        "    x = x_test[i, :, : ] / 255 #normalize pixels, so they will be from 0 to 1\n",
        "\n",
        "    y1 = conv2d_forward_many(x, Conv1_cl, pad=(1,1)) #applying matrix multiplication with the first weight matrix\n",
        "    y2 = y1 + b1_cl #adding bias\n",
        "    y3 = RelU_forward_prop(y2) #applying RelU\n",
        "    y4 =  conv2d_forward_many(y3, Conv2_cl, pad=(1,1)) #applying matrix multiplication with the second weight matrix\n",
        "    y5 = y4 + b2_cl #adding bias\n",
        "    y6 = RelU_forward_prop(y5) #applying RelU\n",
        "\n",
        "    y7 = np.reshape(y6, (vectorized_len, 1))\n",
        "\n",
        "    y8 = MatMul_forward_prop(W1_cl, y7) #applying matrix multiplication with the first weight matrix\n",
        "    y9 = y8 + b3_cl #adding bias\n",
        "    y10 = RelU_forward_prop(y9) #applying RelU\n",
        "    y11 =  MatMul_forward_prop(W2_cl, y10) #applying matrix multiplication with the second weight matrix\n",
        "    y12 = y11 + b4_cl #adding bias\n",
        "    out = SoftMax_forward_prop(y12, True) #applying softmax to find outputs\n",
        "\n",
        "    y_pred[i] = np.argmax(out) #setting prediction (the greatest number among outputs)\n",
        "print(\"\\nAccuracy using L1 distance:\", metrics.accuracy_score(y_true=(y_classes_test - 1), y_pred=y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53mbINkEok2v",
        "outputId": "e88467c9-e049-4ff8-fde4-fedd3d5f7842"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [00:08<00:00,  1.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy using L1 distance: 0.6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Localization model"
      ],
      "metadata": {
        "id": "fu0oCqjVck3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For evaluation I will use intersection over union. The implementation is taken from this [source](https://pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/)"
      ],
      "metadata": {
        "id": "a2SoMfrO6Yhx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bb_intersection_over_union(boxA, boxB):\n",
        "\t# determine the (x, y)-coordinates of the intersection rectangle\n",
        "\txA = max(boxA[0], boxB[0])\n",
        "\tyA = max(boxA[1], boxB[1])\n",
        "\txB = min(boxA[2], boxB[2])\n",
        "\tyB = min(boxA[3], boxB[3])\n",
        "\t# compute the area of intersection rectangle\n",
        "\tinterArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
        "\t# compute the area of both the prediction and ground-truth\n",
        "\t# rectangles\n",
        "\tboxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
        "\tboxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
        "\t# compute the intersection over union by taking the intersection\n",
        "\t# area and dividing it by the sum of prediction + ground-truth\n",
        "\t# areas - the interesection area\n",
        "\tiou = interArea / float(boxAArea + boxBArea - interArea)\n",
        "\t# return the intersection over union value\n",
        "\treturn iou"
      ],
      "metadata": {
        "id": "Vr5inm6Y58lV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_iou = 0\n",
        "total_loss = 0\n",
        "for i in tqdm(range(N_test)):\n",
        "    x = x_test[i, :, :] / 255 #normalize pixels, so they will be from 0 to 1\n",
        "    y_true = y_test[i].reshape(c, 1) / 64\n",
        "\n",
        "\n",
        "    y1 = conv2d_forward_many(x, Conv1_loc, pad=pad) #applying convoltion filters from layer 1\n",
        "    y2 = y1 + b1_loc #adding bias\n",
        "    y3 = RelU_forward_prop(y2) #applying RelU\n",
        "    y4 =  conv2d_forward_many(y3, Conv2_loc, pad=pad) #applying convoltion filters from layer 2\n",
        "    y5 = y4 + b2_loc #adding bias\n",
        "    y6 = RelU_forward_prop(y5) #applying RelU\n",
        "\n",
        "    y7 = np.reshape(y6, (vectorized_len, 1))\n",
        "\n",
        "    y8 = MatMul_forward_prop(W1_loc, y7) #applying matrix multiplication with the first weight matrix\n",
        "    y9 = y8 + b3_loc #adding bias\n",
        "    y10 = RelU_forward_prop(y9) #applying RelU\n",
        "    y11 =  MatMul_forward_prop(W2_loc, y10) #applying matrix multiplication with the second weight matrix\n",
        "    y_pred = y11 + b4_loc #adding bias\n",
        "    # no ReLU after bias addition, since we have nonlinearity further (softmax)\n",
        "    loss = SE_forward(y_pred, y_true)\n",
        "\n",
        "    total_loss += loss #adding current loss to total loss\n",
        "\n",
        "    # Define two bounding boxes as (x, y, w, h)\n",
        "    iou = bb_intersection_over_union(y_pred, y_true)\n",
        "    total_iou += iou"
      ],
      "metadata": {
        "id": "PNyOlH9AyfyZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1531cecc-846e-4ac9-dd2b-4521167a39cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [00:04<00:00,  3.35it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Mean intersection over union:\", total_iou.item() / N_test)\n",
        "print(\"Mean loss:\", total_loss / N_test)"
      ],
      "metadata": {
        "id": "4yZBUjEpymy_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28b18341-00c1-4ffa-d4cb-271c777f60c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean intersection over union: 0.6390189333963597\n",
            "Mean loss: 0.17675812375175765\n"
          ]
        }
      ]
    }
  ]
}