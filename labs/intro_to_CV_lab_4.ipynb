{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0d4L4I8rhk6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def SoftMax_forward(input, normalization=False):\n",
        "  outputs = {} #dictionary of outputs layers\n",
        "  outputs['input'] = np.array(input) #first layer is input\n",
        "  if normalization: #if normalization required\n",
        "    norm = np.array([np.max(input)] * len(input)) #find maximal value\n",
        "    norm = input - norm #subtract from each value, so maximal value after exponent will be 1\n",
        "    outputs['input'] = np.array(norm) #change input to normalized one\n",
        "  outputs['exp'] = np.exp(outputs['input']) #take exponent in power of input for each value\n",
        "  outputs['sum'] = np.array([np.sum(outputs['exp'])] * len(outputs['exp'])) #find sum of exponent values\n",
        "  outputs['div'] = 1 / outputs['sum'] # division by sum for each exponent value\n",
        "  outputs['out'] = outputs['exp'] * outputs['div'] #multiplication of each value and division result\n",
        "  return outputs #outputs is list of output for each layer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loss function required for backward propagation\n",
        "#in my case I use function that returns array with 1 for biggest number, and 0s for others\n",
        "def loss_fn(array):\n",
        "  max_val = np.argmax(array)\n",
        "  out = np.zeros(len(array))\n",
        "  out[max_val] = 1\n",
        "  return out"
      ],
      "metadata": {
        "id": "3hz3gT_G-bYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def SoftMax_backward(input, loss_fn, normalization=False):\n",
        "  outputs = SoftMax_forward(input, normalization) #applying forward propagation\n",
        "  bw_outputs = [] #list of outputs\n",
        "  bw_outputs.append(loss_fn(outputs['out'])) #calculating loss for each output\n",
        "  bw_outputs.append(np.max(outputs['out'] / outputs['div'] * bw_outputs[0])) #backward from out to 1/x\n",
        "  bw_outputs.append(bw_outputs[-1] * (-1 / (outputs['sum'] * outputs['sum']))) # backward from 1/x to copy\n",
        "  bw_outputs.append(outputs['out'] / outputs['exp'] * bw_outputs[0]) #backward to copy from out\n",
        "  bw_outputs.append(bw_outputs[-1]+bw_outputs[-2]) #backward from copy to exp\n",
        "  bw_outputs.append(bw_outputs[-1] * np.exp(outputs['input'])) #backward to input\n",
        "  return bw_outputs #list of backwards for each layer\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GnX0HGS9xB5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Let's test these functions"
      ],
      "metadata": {
        "id": "qxchmtPlnhnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "a = [1., 2., 3.]\n",
        "f = SoftMax_forward(a)\n",
        "b = SoftMax_backward(a, loss_fn)\n",
        "a = torch.tensor(a, requires_grad=True)"
      ],
      "metadata": {
        "id": "B6TdBsiY-UVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Mine softmax forward output:', f['out'])\n",
        "print('Torch softmax forward output:', nn.Softmax(dim=0).forward(a).tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LDkJjjAoZTF",
        "outputId": "77567bf0-4783-49f4-b119-0832671e7a72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mine softmax forward output: [0.09003057 0.24472847 0.66524096]\n",
            "Torch softmax forward output: [0.09003057330846786, 0.2447284758090973, 0.6652409434318542]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss computed by loss function"
      ],
      "metadata": {
        "id": "kznUlguyperp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = loss_fn(f['out'])\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HR41Je1RHijP",
        "outputId": "efd697b3-9036-4b0d-aa82-b39f81413135"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = torch.tensor(loss)\n",
        "res = nn.functional.softmax(a, dim=0).backward(loss)\n",
        "print('Mine softmax backward output:', b[-1])\n",
        "print('Torch softmax backward output:', a.grad.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoozYdLhFa_E",
        "outputId": "2ecdb776-b7ad-446e-a97a-8c74c27dbf30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mine softmax backward output: [-0.05989202 -0.1628034   0.22269543]\n",
            "Torch softmax backward output: [-0.05989202484488487, -0.16280339658260345, 0.22269542515277863]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "a = ['str', 'erere', 'aqeqeaa', 'a']\n",
        "len(max(a, key=len))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMBY-t4oEpT2",
        "outputId": "d7d27bb9-0f0a-4ac2-dee2-eebf2a9f68f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}