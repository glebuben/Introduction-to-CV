{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyURdMUodmv_"
      },
      "outputs": [],
      "source": [
        "#importing everything required for the lab\n",
        "import numpy as np\n",
        "import time\n",
        "from tensorflow import keras\n",
        "from tqdm import tqdm\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading cifar10 dataset\n",
        "dataset=keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = dataset.load_data() #loading data\n",
        "c = 10 # number of labels"
      ],
      "metadata": {
        "id": "mV-dy--sr1AL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cd12fb1-cb6d-44ac-d60b-abcb22d85c67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#function taken from previous assignments\n",
        "#it translates label number into the vector of 0s and 1s\n",
        "def label_vec_func(labels):\n",
        "  labels_matrix = np.zeros([len(labels), c])\n",
        "  for i in range(len(labels)):\n",
        "    labels_matrix[i, labels[i]] = 1\n",
        "  return labels_matrix"
      ],
      "metadata": {
        "id": "AQE2xMQgtgof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 28 * 28 #number of pixels for one picture\n",
        "N_train = 60000 #number of pictures in the train dataset\n",
        "N_test = 10000 #number of pictures in the test dataset\n",
        "x_train = x_train.reshape(N_train, n, 1)\n",
        "x_test = x_test.reshape(N_test, n, 1)"
      ],
      "metadata": {
        "id": "oG68QBDasSDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = label_vec_func(y_train) #converting labels to vector of 0s and 1s"
      ],
      "metadata": {
        "id": "Nhz56bZztkfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#functions for matmul backward and forward propagation from previous assignments\n",
        "def MatMul_forward_prop(matrix, input):\n",
        "  return np.array(matrix) @ np.array(input)\n",
        "\n",
        "#function that finds dL/dx\n",
        "def MatMul_backward_prop(matrix, loss):\n",
        "  return np.array(matrix).T @ np.array(loss)\n",
        "\n",
        "#function that finds dL/dW\n",
        "def MatMul_matrix_backward_prop(X, loss):\n",
        "  return np.array(loss) @ np.array(X).T"
      ],
      "metadata": {
        "id": "l6v9ISjRykSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function for softmax forward and backward propagation\n",
        "#taken from previous assignments\n",
        "def SoftMax_forward_prop(input, normalization=False):\n",
        "  output = np.array(input, dtype=np.longdouble)\n",
        "  if normalization: # if we use normalization\n",
        "    output = output - np.max(input) # we substract maximal value from each number\n",
        "  output = np.exp(output)\n",
        "  return output / np.sum(output)\n",
        "\n",
        "def SoftMax_jacobian(input, normalization=False): # function for calculating jacobian of SoftMax according to input\n",
        "  output = SoftMax_forward_prop(input, normalization)\n",
        "  jacobian = np.zeros((len(input), len(input)))\n",
        "  for i in range(len(input)):\n",
        "    for j in range(len(input)):\n",
        "      if i == j:\n",
        "        jacobian[i][j] = output[i] * (1 - output[j])\n",
        "      else:\n",
        "        jacobian[i][j] = -output[i] * output[j]\n",
        "  return jacobian\n",
        "\n",
        "def SoftMax_backward_prop(input, loss, normalization=False): # backpropagation\n",
        "  jac = SoftMax_jacobian(input, normalization) # calculating jacobian\n",
        "  return jac @ np.array(loss)"
      ],
      "metadata": {
        "id": "YcJ-cKvUzOj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#functions for log_softmax forward and backward propagation\n",
        "#this node applies softmax and then finds logorithm of the result\n",
        "def log_softmax(x):\n",
        "  x_max = np.max(x)\n",
        "  return x - x_max - np.log(np.sum(np.exp(x - x_max)))\n",
        "\n",
        "def log_softmax_jacobian(input): # function for calculating jacobian of SoftMax according to input\n",
        "  output = SoftMax_forward_prop(input, True)\n",
        "  jacobian = np.zeros((len(input), len(input)))\n",
        "  for i in range(len(input)):\n",
        "    for j in range(len(input)):\n",
        "      if i == j:\n",
        "        jacobian[i][j] = (1 - output[j])\n",
        "      else:\n",
        "        jacobian[i][j] = -output[j]\n",
        "  return jacobian\n",
        "\n",
        "def log_softmax_backward_prop(input, loss): # backpropagation\n",
        "  jac = log_softmax_jacobian(input) # calculating jacobian\n",
        "  return jac @ np.array(loss)"
      ],
      "metadata": {
        "id": "3IEIGdDseUmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function for RelU forward and backward propagation\n",
        "#taken from previous assignments\n",
        "def RelU_jacobian(input):\n",
        "  jac = np.zeros((len(input), len(input)))\n",
        "  for i in range(len(input)):\n",
        "    if input[i] > 0:\n",
        "      jac[i][i] = 1\n",
        "  return jac\n",
        "\n",
        "def RelU_forward_prop(input):\n",
        "  output = np.array([max(0, x.item()) for x in input]) # applying RelU to the input\n",
        "  return np.expand_dims(output, axis=1)\n",
        "\n",
        "def RelU_backward_prop(input, loss):\n",
        "  jac = RelU_jacobian(input) # finding jacobian for RelU according to input\n",
        "  return jac @ np.array(loss)"
      ],
      "metadata": {
        "id": "-MnNPpSDymzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Crucial formula for backward propagation\n",
        "This will be used in both neural networks during backward propagation"
      ],
      "metadata": {
        "id": "xp4PYp2H1PEK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\\frac{\\partial L}{\\partial s_{in}} = s_{out} - y_{true}$$\n",
        "\n",
        "Where $s_{in}$ is the input of softmax layer, $s_{out}$ is the output of softmax layer, $y_{true}$ is a vector of true label (1 at the index equal to the label number, and 0s at other indices)"
      ],
      "metadata": {
        "id": "3bGQ92O94gzH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##source:\n",
        "https://www.mldawn.com/back-propagation-with-cross-entropy-and-softmax/"
      ],
      "metadata": {
        "id": "hiA6V-q55qsU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#One-layer neural network"
      ],
      "metadata": {
        "id": "uJ1gBzESv3UQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training"
      ],
      "metadata": {
        "id": "TYvXr2rBxT-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W = np.random.uniform(0, 1, (c, n)) #W_0 - initial weights\n",
        "nu = 0.18 # learning rate\n",
        "num_epochs = 10 # amount of epochs\n",
        "\n",
        "for i in range(num_epochs): #for each epoch\n",
        "  total_loss = 0 #sum of losses for one epoch\n",
        "  correct = 0 #number of correct predictions\n",
        "  for i in tqdm(range(N_train)): #for each picture\n",
        "\n",
        "    x = x_train[i] / 255 #normalize pixels, so they will be from 0 to 1\n",
        "\n",
        "    y_true = np.array(y_train[i].reshape(c,  1)) #true value of y\n",
        "\n",
        "    #forward propagation\n",
        "\n",
        "    y1 = MatMul_forward_prop(W, x) #applying matrix multiplication\n",
        "    y2 = log_softmax(y1) #applying log softmax\n",
        "\n",
        "    if np.argmax(y2) == np.argmax(y_true): #check if predictions are correct\n",
        "      correct += 1\n",
        "\n",
        "    loss = y_true.T @ y2 #finding loss\n",
        "    total_loss += loss #adding current loss to total loss\n",
        "\n",
        "    #backward propagation\n",
        "\n",
        "    back = -y_true + SoftMax_forward_prop(y1, True) #backpropagation from loss to the input of softmax\n",
        "\n",
        "    back = MatMul_matrix_backward_prop(x, back) #fiding dL/dW\n",
        "    dL_dW = back\n",
        "\n",
        "    #applying gradient descent\n",
        "\n",
        "    W = W - nu * dL_dW\n",
        "\n",
        "  print('\\nAccuracy:', correct / N_train)\n",
        "  print('Loss:', -total_loss.item() / N_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "bBsgXkYzyFql",
        "outputId": "3ef852ca-6dc7-4c8c-88ac-8f73fc1753ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/60000 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(784, 1)\n",
            "\n",
            "Accuracy: 0.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-2b4b15a58859>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nAccuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mN_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mN_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'item'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testing"
      ],
      "metadata": {
        "id": "RocyZDVsxSPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = np.zeros((10000, 1), int) #predictions\n",
        "for i in tqdm(range(N_test)):\n",
        "    x = x_test[i]\n",
        "\n",
        "    y1 = MatMul_forward_prop(W, x) #applying matrix multiplication\n",
        "    out = SoftMax_forward_prop(y1, True) #finding predictions\n",
        "\n",
        "    y_pred[i] = np.argmax(out) #setting prediction (the greatest number among outputs)"
      ],
      "metadata": {
        "id": "uyr55DgCHrZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy using L1 distance:\", metrics.accuracy_score(y_true=y_test, y_pred=y_pred))"
      ],
      "metadata": {
        "id": "SNCJDnJTIU5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Two-layer neural network"
      ],
      "metadata": {
        "id": "yb7vd00oxhBN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training"
      ],
      "metadata": {
        "id": "qMYDqoqNxp6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c1 = 14 * 14 # dimension of the first layer\n",
        "W1 = np.random.uniform(-1, 1, (c1, n)) #W1 - initial weights\n",
        "W2 = np.random.uniform(-1, 1, (c, c1)) #W2 - initial weights\n",
        "b1 = np.random.uniform(-1, 1 , (c1, 1)) #b1 - initial bias\n",
        "b2 = np.random.uniform(-1, 1 , (c, 1)) #b2 - initial bias\n",
        "nu = 0.001 # learning rate\n",
        "num_epochs = 10 # amount of epochs\n",
        "\n",
        "for i in range(num_epochs): #for each epoch\n",
        "  total_loss = 0 #sum of losses for one epoch\n",
        "  correct = 0 #number of correct predictions\n",
        "  for i in tqdm(range(N_train)): #for each picture\n",
        "\n",
        "    x = x_train[i] / 255 #normalize pixels, so they will be from 0 to 1\n",
        "\n",
        "    y_true = np.array(y_train[i].reshape(c,  1))  #true value of y\n",
        "\n",
        "    #forward propagation\n",
        "\n",
        "    y1 = MatMul_forward_prop(W1, x) #applying matrix multiplication with the first weight matrix\n",
        "    y2 = y1 + b1 #adding bias\n",
        "    y3 = RelU_forward_prop(y2) #applying RelU\n",
        "    y4 =  MatMul_forward_prop(W2, y3) #applying matrix multiplication with the second weight matrix\n",
        "    y5 = y4 + b2 #adding bias\n",
        "    y6 = log_softmax(y5) #applying log softmax\n",
        "\n",
        "    if np.argmax(y6) == np.argmax(y_true): #check if predictions are correct\n",
        "      correct += 1\n",
        "\n",
        "    loss = y_true.T @ y6 #finding loss\n",
        "    total_loss += loss #adding current loss to total loss\n",
        "\n",
        "    #backward propagation\n",
        "\n",
        "    back = -y_true + SoftMax_forward_prop(y5, True) #backpropagation from loss to the input of softmax\n",
        "\n",
        "    dL_db2 = back #backpropagation from loss to the input of addition, finding dL/db2\n",
        "\n",
        "    back = MatMul_backward_prop(W2, back) #backpropagation from loss to the input of matrix multiplication with matrix W2\n",
        "\n",
        "    dL_dW2 = MatMul_matrix_backward_prop(y3, back) #fiding dL/dW2\n",
        "\n",
        "    back = RelU_backward_prop(y2, back) #backpropagation from loss to the input of RelU\n",
        "\n",
        "    dL_db1 = back #backpropagation from loss to the input of addition, finding dL/db1\n",
        "\n",
        "    dL_dW1 = MatMul_matrix_backward_prop(x, back) #fiding dL/dW1\n",
        "\n",
        "    #applying gradient descent for weights and biases\n",
        "\n",
        "    W1 = W1 - nu * dL_dW1\n",
        "    W2 = W2 - nu * dL_dW2\n",
        "    b1 = b1 - nu * dL_db1\n",
        "    b2 = b2 - nu * dL_db2\n",
        "\n",
        "\n",
        "  print('\\nAccuracy:', correct / N_train)\n",
        "  print('Loss:', -total_loss.item() / N_train)\n"
      ],
      "metadata": {
        "id": "xk61MAXPxaxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testing"
      ],
      "metadata": {
        "id": "8peCNK6uxr0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = np.zeros((10000, 1), int) #predictions\n",
        "for i in tqdm(range(N_test)):\n",
        "    x = x_test[i]\n",
        "\n",
        "    y1 = MatMul_forward_prop(W1, x) #applying matrix multiplication with the first weight matrix\n",
        "    y2 = y1 + b1 #adding bias\n",
        "    y3 = RelU_forward_prop(y2) #applying RelU\n",
        "    y4 =  MatMul_forward_prop(W2, y3) #applying matrix multiplication with the second weight matrix\n",
        "    y5 = y4 + b2 #adding bias\n",
        "    out = SoftMax_forward_prop(y5, True) #applying softmax to find outputs\n",
        "\n",
        "    y_pred[i] = np.argmax(out) #setting prediction (the greatest number among outputs)"
      ],
      "metadata": {
        "id": "gFq05f8nQ37f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy using L1 distance:\", metrics.accuracy_score(y_true=y_test, y_pred=y_pred))"
      ],
      "metadata": {
        "id": "CCsFGqP5Q5WI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}