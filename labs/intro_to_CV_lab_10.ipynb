{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "wn7bC7OIqBbN",
        "aukHJSXSvxpG"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEhmtS9kDy60"
      },
      "outputs": [],
      "source": [
        "#importing everything required for the lab\n",
        "import numpy as np\n",
        "import time\n",
        "from tensorflow import keras\n",
        "from tqdm import tqdm\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2"
      ],
      "metadata": {
        "id": "8o7unZZIg2SW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def compute_pad_filt(input_size, output_size):\n",
        "  Hx, Wx = input_size\n",
        "  Hy, Wy = output_size\n",
        "\n",
        "  min_pad_h, min_pad_w = 1, 1 # minimal padding is (1, 1)\n",
        "\n",
        "  min_h, min_w = Hx-Hy+3, Wx-Wy+3 # for minimal padding kernel size is: min_h = Hx - Hy + 2 * pad[0] + 1, min_w = Wx - Wy + 2 * pad[1] + 1\n",
        "\n",
        "  if min_h < 1: # minimal size for height of a kernel = 1\n",
        "    min_pad_h = 1 + math.ceil((1-min_h) / 2) # add paddings so kernel height greater or equal to 1\n",
        "    min_h = min_h + (min_pad_h-1) * 2 # calculate new height\n",
        "\n",
        "\n",
        "  if min_w < 1: # minimal size for width of a kernel = 1\n",
        "    min_pad_w = 1 + math.ceil((1-min_w) / 2) # add paddings so kernel width greater or equal to 1\n",
        "    min_w = min_w + (min_pad_w-1) * 2  # calculate new height\n",
        "\n",
        "\n",
        "  return (min_pad_h, min_pad_w), (min_h, min_w)"
      ],
      "metadata": {
        "id": "R28ooVrNgtFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Copying functions from previous labs"
      ],
      "metadata": {
        "id": "BKkTDKTjJYoJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolution"
      ],
      "metadata": {
        "id": "BWemJAqvJhem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# forward propagation for convolution\n",
        "# add padings\n",
        "def conv2d_forward(matrix, filter, pad=(0,0)):\n",
        "  if pad != (0,0): # if there are paddings, apply them\n",
        "    matrix = np.pad(matrix, ((pad[0], pad[0]),(pad[1], pad[1]),(0,0)))\n",
        "  h_x, w_x, _ = matrix.shape # getting matrix shape\n",
        "  h_w, w_w, _  = filter.shape # getting filter shape\n",
        "  output = np.zeros((h_x - h_w + 1, w_x - w_w + 1)) # initializing output matrix\n",
        "  for i in range(len(output)): # for each pixel\n",
        "    for j in range(len(output[i])):\n",
        "        output[i][j] = np.sum(matrix[i:i+h_w, j:j+w_w, :] * filter) # calculate sum of hadamart product between\n",
        "                                                                    # matrix batch and filter, save value in output cell\n",
        "  return output"
      ],
      "metadata": {
        "id": "4jkHBWEmJXaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# backward propagation for convolution (dL/dZ)\n",
        "def conv2d_backward_pad(upstream, filter, pad=(0,0)):\n",
        "  # if there are paddings, we send them to conv2d_forward\n",
        "  h_w, w_w, d_w  = filter.shape # getting filter shape\n",
        "  rotated_filter = np.rot90(np.rot90(filter)) # rotate filter by 180 degree\n",
        "  dL_dZ = [] # initializing output\n",
        "  for i in range(d_w): # for each channel\n",
        "    dL_dZ.append(conv2d_forward(upstream, rotated_filter[:, :, i, np.newaxis], pad)) # adding dL/dZ\n",
        "  return np.array(dL_dZ)"
      ],
      "metadata": {
        "id": "Wt-vYIhvJlSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# backward propagation for convolution (dL/dW)\n",
        "def conv2d_backward_weights(weights, upstream, pad=(0,0)):\n",
        "   # if there are paddings, apply them\n",
        "  if pad != (0,0):\n",
        "    weights = np.pad(weights, ((pad[0], pad[0]),(pad[1], pad[1]),(0,0)))\n",
        "  h_x, w_x, d_x  = weights.shape # getting filter shape\n",
        "  dL_dZ = [] # initializing output\n",
        "  for i in range(d_x): # for each channel\n",
        "    dL_dZ.append(conv2d_forward(weights[:, :, i, np.newaxis], upstream)) # adding dL/dZ\n",
        "  return np.transpose(np.array(dL_dZ), (1, 2, 0))"
      ],
      "metadata": {
        "id": "NlsBN9tqJnBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ReLU"
      ],
      "metadata": {
        "id": "CuF8nrrlJnd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function for RelU forward and backward propagation\n",
        "#taken from previous assignments\n",
        "def RelU_jacobian(input):\n",
        "  return 1 * (input > 0)\n",
        "\n",
        "def RelU_forward_prop(input):\n",
        "  return np.maximum(input, 0)\n",
        "\n",
        "def RelU_backward_prop(input, loss):\n",
        "  jac = RelU_jacobian(input) # finding jacobian for RelU according to input\n",
        "  return jac * np.array(loss)"
      ],
      "metadata": {
        "id": "H3Cz0h1LJ69I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matmul"
      ],
      "metadata": {
        "id": "jPLv8BwuJ27M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#functions for matmul backward and forward propagation from previous assignments\n",
        "def MatMul_forward_prop(matrix, input):\n",
        "  return np.array(matrix) @ np.array(input)\n",
        "\n",
        "#function that finds dL/dx\n",
        "def MatMul_backward_prop(matrix, loss):\n",
        "  return np.array(matrix).T @ np.array(loss)\n",
        "\n",
        "#function that finds dL/dW\n",
        "def MatMul_matrix_backward_prop(X, loss):\n",
        "  return np.array(loss) @ np.array(X).T"
      ],
      "metadata": {
        "id": "xsFTVfkDJ4do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SoftMax"
      ],
      "metadata": {
        "id": "jIM6vuJBJ7mg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function for softmax forward and backward propagation\n",
        "#taken from previous assignments\n",
        "def SoftMax_forward_prop(input, normalization=False):\n",
        "  output = np.array(input, dtype=np.longdouble)\n",
        "  if normalization: # if we use normalization\n",
        "    output = output - np.max(input) # we substract maximal value from each number\n",
        "  output = np.exp(output)\n",
        "  return output / np.sum(output)\n",
        "\n",
        "def SoftMax_jacobian(input, normalization=False): # function for calculating jacobian of SoftMax according to input\n",
        "  output = SoftMax_forward_prop(input, normalization)\n",
        "  jacobian = np.zeros((len(input), len(input)))\n",
        "  for i in range(len(input)):\n",
        "    for j in range(len(input)):\n",
        "      if i == j:\n",
        "        jacobian[i][j] = output[i] * (1 - output[j])\n",
        "      else:\n",
        "        jacobian[i][j] = -output[i] * output[j]\n",
        "  return jacobian\n",
        "\n",
        "def SoftMax_backward_prop(input, loss, normalization=False): # backpropagation\n",
        "  jac = SoftMax_jacobian(input, normalization) # calculating jacobian\n",
        "  return jac @ np.array(loss)"
      ],
      "metadata": {
        "id": "KFWemZTUJ9vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Log softmax"
      ],
      "metadata": {
        "id": "KIv7o1k4J_4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#functions for log_softmax forward and backward propagation\n",
        "#this node applies softmax and then finds logorithm of the result\n",
        "def log_softmax(x):\n",
        "  x_max = np.max(x)\n",
        "  return x - x_max - np.log(np.sum(np.exp(x - x_max)))\n",
        "\n",
        "def log_softmax_jacobian(input): # function for calculating jacobian of SoftMax according to input\n",
        "  output = SoftMax_forward_prop(input, True)\n",
        "  jacobian = np.zeros((len(input), len(input)))\n",
        "  for i in range(len(input)):\n",
        "    for j in range(len(input)):\n",
        "      if i == j:\n",
        "        jacobian[i][j] = (1 - output[j])\n",
        "      else:\n",
        "        jacobian[i][j] = -output[j]\n",
        "  return jacobian\n",
        "\n",
        "def log_softmax_backward_prop(input, loss): # backpropagation\n",
        "  jac = log_softmax_jacobian(input) # calculating jacobian\n",
        "  return jac @ np.array(loss)"
      ],
      "metadata": {
        "id": "jfppFkmSKCDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Labels vectorization"
      ],
      "metadata": {
        "id": "5k5dKRqdcFvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#function taken from previous assignments\n",
        "#it translates label number into the vector of 0s and 1s\n",
        "def label_vec_func(labels):\n",
        "  labels_matrix = np.zeros([len(labels), c])\n",
        "  for i in range(len(labels)):\n",
        "    labels_matrix[i, labels[i]] = 1\n",
        "  return labels_matrix"
      ],
      "metadata": {
        "id": "565Ye-qfROCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing functions required for this lab"
      ],
      "metadata": {
        "id": "_c9vGHcEb6GJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# forward propagation for convolution\n",
        "# now we have filters - array of size: (height, width, depth, number of filters)\n",
        "# add paddings to the function calls\n",
        "def conv2d_forward_many(matrix, filters, pad=(0,0)):\n",
        "  h_x, w_x, _ = matrix.shape # getting matrix shape\n",
        "  h_w, w_w, _, d  = filters.shape # getting filter shape\n",
        "  output = np.zeros((h_x - h_w + 1 + 2 *pad[0], w_x - w_w + 1 + 2 * pad[1], d)) # initializing output matrix\n",
        "  for k in range(d): #for each filter\n",
        "    output[:, :, k] = conv2d_forward(matrix, filters[:, :, :, k], pad)\n",
        "  return output"
      ],
      "metadata": {
        "id": "dZEgEccRcBHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# backward propagation for convolution (dL/dZ)\n",
        "# now we have filters - array of size: (height, width, depth, number of filters)\n",
        "# add paddings to the function calls\n",
        "def conv2d_backward_many(upstream, filters, pad=(0,0)):\n",
        "  h_w, w_w, d_w, D  = filters.shape # getting filter shape\n",
        "  dL_dZ = []\n",
        "  for i in range(D):  # for each filter\n",
        "    dL_dZ.append(conv2d_backward_pad(upstream[:, :, i, np.newaxis], filters[:, :, : , i], pad))\n",
        "  return np.transpose(np.sum(dL_dZ, 0), (1, 2, 0))"
      ],
      "metadata": {
        "id": "_2_VDSVeqhPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# backward propagation for convolution (dL/dW)\n",
        "# now we have filters - array of size: (height, width, depth, number of filters)\n",
        "# add paddings to the function calls\n",
        "def conv2d_backward_weights_many(weight, upstream, pad=(0,0)):\n",
        "  _, _, D  = upstream.shape # getting filter shape\n",
        "  dL_dWs = [] # initializing output\n",
        "  for i in range(D): # for each channel\n",
        "    dL_dWs.append(conv2d_backward_weights(weight, upstream[:, :, i, np.newaxis], pad))\n",
        "  return np.transpose(np.array(dL_dWs), (1, 2, 3, 0))"
      ],
      "metadata": {
        "id": "GEKpsqIdqlvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading and preprocessing dataset"
      ],
      "metadata": {
        "id": "MOnFAhPQXyjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading cifar10 dataset\n",
        "dataset=keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = dataset.load_data() #loading data\n",
        "c = 5 # number of labels, I will use 5"
      ],
      "metadata": {
        "id": "uJm04JOcM53b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's decrease samples in dataset in order to speed up the training process"
      ],
      "metadata": {
        "id": "nrRwIVJ9Y5dV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decrease_mnist_samples(data, new_num_classes, max_repeat):\n",
        "  # decreasing dataset by number of classes and number of samples for each class\n",
        "  classes = range(new_num_classes) # classes that stayed\n",
        "  repeations = {i:0 for i in classes} # dictionary for repetitions\n",
        "  decreased_X, decreased_Y = [], [] # new X and Y\n",
        "  for x, y in data:\n",
        "    if y in classes: # for remained class\n",
        "      if repeations[y] < max_repeat: # check number of repetitions\n",
        "        repeations[y]+=1 # update this number\n",
        "        decreased_X.append(x) # add new value to X\n",
        "        decreased_Y.append(y) # add new value to Y\n",
        "  # shuffle new dataset\n",
        "  np.random.shuffle(decreased_X)\n",
        "  np.random.shuffle(decreased_Y)\n",
        "  return np.array(decreased_X), np.array(decreased_Y)"
      ],
      "metadata": {
        "id": "cIDvcrXtY4kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train)= decrease_mnist_samples(zip(x_train, y_train), c, 160)\n",
        "(x_test, y_test) = decrease_mnist_samples(zip(x_test, y_test), c, 20)\n",
        "y_train = label_vec_func(y_train) #converting labels to vector of 0s and 1s\n",
        "N_train = y_train.shape[0] #number of pictures in the train dataset\n",
        "N_test = y_test.shape[0] #number of pictures in the test dataset"
      ],
      "metadata": {
        "id": "YFM0k-yzadC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOUZkMleM0RW",
        "outputId": "553de4d3-cb9c-4a79-816d-c52442159859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(800, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "h, w = 28, 28\n",
        "n = h * w #number of pixels for one picture"
      ],
      "metadata": {
        "id": "4AsA5JUEn63x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pad, kernel = compute_pad_filt((28, 28), (28, 28)) # we want output to have the smae dimensions\n",
        "\n",
        "hw1, Ww1  = kernel # initialize kernel sizes for first layer\n",
        "d, D1 = 1, 2 # initialize depth and number of filters for first layer\n",
        "\n",
        "hw2, Ww2, D2 = hw1, Ww1, 2 # initialize kernel sizes, number of filters for second layer\n",
        "\n",
        "Conv1 =  np.random.uniform(-1, 1, (hw1, Ww1, d, D1)) # initial convolution filters for layer 1\n",
        "b1 = np.random.uniform(-1, 1 , (28, 28, D1)) #b1 - initial bias\n",
        "Conv2 =  np.random.uniform(-1, 1, (hw2, Ww2, D1, D2))# initial convolution filters for layer 2\n",
        "b2 = np.random.uniform(-1, 1 , (28, 28, D2)) #b2 - initial bias\n",
        "\n",
        "vectorized_len = 28 * 28 * D2 # size for vectorized tensor after 2 convulitions\n",
        "\n",
        "c1 = vectorized_len // 4 # dimension of the first layer\n",
        "W1 = np.random.uniform(-1, 1, (c1, vectorized_len)) #W1 - initial weights\n",
        "W2 = np.random.uniform(-1, 1, (c, c1)) #W2 - initial weights\n",
        "b3 = np.random.uniform(-1, 1 , (c1, 1)) #b1 - initial bias\n",
        "b4 = np.random.uniform(-1, 1 , (c, 1)) #b2 - initial bias\n",
        "\n",
        "\n",
        "nu = 0.005 # learning rate\n",
        "num_epochs = 10 # amount of epochs\n",
        "\n",
        "N = 2 # number of images in minibatch\n",
        "\n",
        "# initialize partial derivatives\n",
        "dL_dConv1 = np.zeros((28, 28, D1))\n",
        "dL_dConv2 = np.zeros((hw1, Ww1, d, D1))\n",
        "dL_dW2 = np.zeros((c1, vectorized_len))\n",
        "dL_dW2 = np.zeros((c, c1))\n",
        "dL_db1 = 0\n",
        "dL_db2 = 0\n",
        "dL_db3 = 0\n",
        "dL_db4 = 0"
      ],
      "metadata": {
        "id": "-XQDgSU-nQwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(num_epochs): #for each epoch\n",
        "  total_loss = 0 #sum of losses for one epoch\n",
        "  correct = 0 #number of correct predictions\n",
        "  counter = 0 #counter to check that batch ended\n",
        "  for i in tqdm(range(N_train)): #for each picture\n",
        "\n",
        "    x = x_train[i, :, : , np.newaxis] / 255 #normalize pixels, so they will be from 0 to 1\n",
        "\n",
        "    y_true = np.array(y_train[i].reshape(c,  1))  #true value of y\n",
        "\n",
        "    #forward propagation\n",
        "\n",
        "    y1 = conv2d_forward_many(x, Conv1, pad=pad) #applying convoltion filters from layer 1\n",
        "    y2 = y1 + b1 #adding bias\n",
        "    y3 = RelU_forward_prop(y2) #applying RelU\n",
        "    y4 =  conv2d_forward_many(y3, Conv2, pad=pad) #applying convoltion filters from layer 2\n",
        "    y5 = y4 + b2 #adding bias\n",
        "    y6 = RelU_forward_prop(y5) #applying RelU\n",
        "\n",
        "    y7 = np.reshape(y6, (vectorized_len, 1))\n",
        "\n",
        "    y8 = MatMul_forward_prop(W1, y7) #applying matrix multiplication with the first weight matrix\n",
        "    y9 = y8 + b3 #adding bias\n",
        "    y10 = RelU_forward_prop(y9) #applying RelU\n",
        "    y11 =  MatMul_forward_prop(W2, y10) #applying matrix multiplication with the second weight matrix\n",
        "    y12 = y11 + b4 #adding bias\n",
        "    # no ReLU after bias addition, since we have nonlinearity further (softmax)\n",
        "    y13 = log_softmax(y12) #applying log softmax\n",
        "\n",
        "    if np.argmax(y13) == np.argmax(y_true): #check if predictions are correct\n",
        "      correct += 1\n",
        "\n",
        "    loss = y_true.T @ y13 #finding loss\n",
        "    total_loss += loss #adding current loss to total loss\n",
        "\n",
        "    #backward propagation\n",
        "\n",
        "    back = -y_true + SoftMax_forward_prop(y12, True) #backpropagation from loss to the input of softmax\n",
        "\n",
        "    dL_db4 += back #backpropagation from loss to the input of addition, finding dL/db4\n",
        "\n",
        "    dL_dW2 += MatMul_matrix_backward_prop(y10, back) #fiding dL/dW2\n",
        "\n",
        "    back = MatMul_backward_prop(W2, back) #backpropagation from loss to the input of matrix multiplication with matrix W2\n",
        "\n",
        "    back = RelU_backward_prop(y9, back) #backpropagation from loss to the input of RelU\n",
        "\n",
        "    dL_db3 = back #backpropagation from loss to the input of addition, finding dL/db3\n",
        "\n",
        "    dL_dW1 = MatMul_matrix_backward_prop(y7, back) #fiding dL/dW1\n",
        "\n",
        "    back = MatMul_backward_prop(W1, back) #backpropagation from addition to the input of matrix multiplication with matrix W1\n",
        "\n",
        "    back = back.reshape((28, 28, D2)) # backpropagation of reshaping\n",
        "\n",
        "    back = RelU_backward_prop(y5, back) # backpropagation from reshaping to the input of RelU\n",
        "\n",
        "    dL_db2 = back #backpropagation from ReLU to the input of addition, finding dL/db2\n",
        "\n",
        "    dL_dConv2 = conv2d_backward_weights_many(y3, back, pad=pad) #finding dL/dConv2\n",
        "\n",
        "    back = conv2d_backward_many(back, Conv2, pad=pad) # backpropagation from addition to the input convolution\n",
        "    back = RelU_backward_prop(y2, back) # backpropagation from convolution to the input of RelU\n",
        "\n",
        "    dL_db1 = back #backpropagation from ReLU to the input of addition, finding dL/db1\n",
        "    dL_dConv1 = conv2d_backward_weights_many(x, back, pad=(1,1)) #finding dL/dConv1\n",
        "\n",
        "    counter += 1 # increasing counter\n",
        "\n",
        "    if counter == N or i == N_train - 1: # if batch ended or dataset ended. We apply gradient descent only in batches\n",
        "    #applying gradient descent for weights and biases\n",
        "\n",
        "      Conv1 = Conv1 - nu * dL_dConv1\n",
        "      Conv2 = Conv2 - nu * dL_dConv2\n",
        "      W1 = W1 - nu / N * dL_dW1\n",
        "      W2 = W2 - nu / N * dL_dW2\n",
        "      b1 = b1 - nu / N * dL_db1\n",
        "      b2 = b2 - nu / N * dL_db2\n",
        "      b3 = b3 - nu / N * dL_db3\n",
        "      b4 = b4 - nu / N * dL_db4\n",
        "\n",
        "      # setting partial derivatives to 0\n",
        "      dL_dConv1 = np.zeros((h - hw1 + 1, h - Ww1 + 1, D1))\n",
        "      dL_dConv2 = np.zeros((hw1, Ww1, d, D1))\n",
        "      dL_dW2 = np.zeros((c1, vectorized_len))\n",
        "      dL_dW2 = np.zeros((c, c1))\n",
        "      dL_db1 = 0\n",
        "      dL_db2 = 0\n",
        "      dL_db3 = 0\n",
        "      dL_db4 = 0\n",
        "\n",
        "\n",
        "  print('\\nAccuracy:', correct / N_train)\n",
        "  print('Loss:', -total_loss.item() / N_train)\n"
      ],
      "metadata": {
        "id": "-y1DHX4dnxf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dd9a16f-dc3a-4f3e-f490-7c83f9a3a414"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [01:13<00:00, 10.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.49875\n",
            "Loss: 64.04468725973176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [01:00<00:00, 13.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.5\n",
            "Loss: 312.37500941118197997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:59<00:00, 13.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.5\n",
            "Loss: 167.02876952455233375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [01:01<00:00, 13.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.5\n",
            "Loss: 85.31357321349504172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [01:00<00:00, 13.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.5\n",
            "Loss: 8.489564674793341642\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [01:00<00:00, 13.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.5\n",
            "Loss: 58.123657769172100373\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:59<00:00, 13.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.50125\n",
            "Loss: 4.6230458548955023256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [01:00<00:00, 13.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.5\n",
            "Loss: 44.881545601911478544\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [01:02<00:00, 12.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.5\n",
            "Loss: 19.99798501407944714\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:58<00:00, 13.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 0.5\n",
            "Loss: 30.79328969455031192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation, taken from lab 6"
      ],
      "metadata": {
        "id": "cwinqb2Pm2a3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = np.zeros((N_test, 1), int) #predictions\n",
        "for i in tqdm(range(N_test)):\n",
        "    x = x_test[i, :, : , np.newaxis] / 255 #normalize pixels, so they will be from 0 to 1\n",
        "\n",
        "    y1 = conv2d_forward_many(x, Conv1, pad=(1,1)) #applying matrix multiplication with the first weight matrix\n",
        "    y2 = y1 + b1 #adding bias\n",
        "    y3 = RelU_forward_prop(y2) #applying RelU\n",
        "    y4 =  conv2d_forward_many(y3, Conv2, pad=(1,1)) #applying matrix multiplication with the second weight matrix\n",
        "    y5 = y4 + b2 #adding bias\n",
        "    y6 = RelU_forward_prop(y5) #applying RelU\n",
        "\n",
        "    y7 = np.reshape(y6, (vectorized_len, 1))\n",
        "\n",
        "    y8 = MatMul_forward_prop(W1, y7) #applying matrix multiplication with the first weight matrix\n",
        "    y9 = y8 + b3 #adding bias\n",
        "    y10 = RelU_forward_prop(y9) #applying RelU\n",
        "    y11 =  MatMul_forward_prop(W2, y10) #applying matrix multiplication with the second weight matrix\n",
        "    y12 = y11 + b4 #adding bias\n",
        "    out = SoftMax_forward_prop(12, True) #applying softmax to find outputs\n",
        "\n",
        "    y_pred[i] = np.argmax(out) #setting prediction (the greatest number among outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNyOlH9AyfyZ",
        "outputId": "f99c86e0-e6b2-4e44-aea5-55b93973fb85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:06<00:00, 14.99it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy using L1 distance:\", metrics.accuracy_score(y_true=y_test, y_pred=y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yZBUjEpymy_",
        "outputId": "56566ae2-b198-4d3d-ffcb-fb34cecde56a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy using L1 distance: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Torch based model"
      ],
      "metadata": {
        "id": "VcbEcKK8m6fg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class ConvolutionalNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.sequence = nn.Sequential(\n",
        "        nn.Conv2d(d, D1, (hw1, Ww1), padding=1, bias=True), # Convolution layer 1 with the same parameters and with bias\n",
        "        nn.ReLU(), # ReLU\n",
        "        nn.Conv2d(D1, D2, (hw2, Ww2), padding=1, bias=True), # Convolution layer 2 with the same parameters\n",
        "        nn.ReLU(), # RelU\n",
        "        nn.Flatten(), # Vectorization\n",
        "        nn.Linear(vectorized_len, c1), # Linear layer 1 with the same parameters, has bias by default\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(c1, c), # Linear layer 2 with the same parameters, has bias by default\n",
        "        nn.Softmax(dim=1) # Softmax\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "        return self.sequence(x)"
      ],
      "metadata": {
        "id": "mQDUW6K9U1g8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ConvolutionalNN() # initializing model\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=nu) # gradient descent with same learning rate\n",
        "loss_fn = nn.CrossEntropyLoss() # same loss"
      ],
      "metadata": {
        "id": "tF8z60DShTOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for i in range(num_epochs): #for each epoch\n",
        "  total_loss = 0 #sum of losses for one epoch\n",
        "  X_batch = []\n",
        "  Y_batch = []\n",
        "  for i in tqdm(range(N_train)): #for each picture\n",
        "\n",
        "    x = torch.tensor(x_train[i, np.newaxis, :, :] / 255, dtype=torch.float32)\n",
        "    y = torch.tensor(y_train[i], dtype=torch.float32)\n",
        "\n",
        "    if len(X_batch) > N or i == N_train - 1: # if batch ended\n",
        "      # convert list of samples to tensors\n",
        "      X = torch.stack(X_batch)\n",
        "      Y = torch.stack(Y_batch)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      output = model(X) # find output\n",
        "      loss = loss_fn(output, Y) # send to loss\n",
        "      total_loss += loss.item()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # clear batches\n",
        "      X_batch = []\n",
        "      Y_batch = []\n",
        "\n",
        "    else:\n",
        "      # add to batch\n",
        "      X_batch.append(x)\n",
        "      Y_batch.append(y)\n",
        "\n",
        "  print('Loss:', total_loss / N_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bUPEgE4ieb7",
        "outputId": "877ba2e8-8db9-4cb9-fb6e-193d785b9f26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:00<00:00, 804.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.23060718186199666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:00<00:00, 1036.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.23054377049207686\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:00<00:00, 1080.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.23048127949237823\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:00<00:00, 1045.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.23041169561445712\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:00<00:00, 1041.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.2303383333235979\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:00<00:00, 1038.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.23025355353951454\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:00<00:00, 1060.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.23015364862978457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:00<00:00, 1071.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.23003175839781762\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:00<00:00, 970.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.22987762682139873\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 800/800 [00:01<00:00, 735.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.22967446923255921\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation, taken from lab 6"
      ],
      "metadata": {
        "id": "rbyHhwGDm-iW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = np.zeros((N_test, 1), int) #predictions\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for i in range(N_test): # for each sample in test\n",
        "        x = torch.tensor(x_test[i, :].reshape(1, 1, 28, 28), dtype=torch.float32) # convert to tensor\n",
        "        y_pred[i] = torch.argmax(model(x)) # find output of a model"
      ],
      "metadata": {
        "id": "dRc3ZSH8oFo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy using L1 distance:\", metrics.accuracy_score(y_true=y_test, y_pred=y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gH_yHVDWz-pr",
        "outputId": "e6c7aacf-b86d-4bcb-e2b7-51ab554b1d33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy using L1 distance: 0.51\n"
          ]
        }
      ]
    }
  ]
}